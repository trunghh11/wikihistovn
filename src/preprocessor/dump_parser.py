import bz2
import xml.etree.cElementTree as ET
import mwparserfromhell
import json
import os
import time
from tqdm import tqdm  # th√™m th∆∞ vi·ªán n√†y ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh

def extract_infobox_data(wikicode):
    """
    Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ template Infobox ƒë·∫ßu ti√™n t√¨m th·∫•y.
    """
    infoboxes = wikicode.filter_templates(matches=lambda t: 'infobox' in t.name.lower())
    if not infoboxes:
        return None
    
    infobox = infoboxes[0]
    data = {'template_name': infobox.name.strip()}
    for param in infobox.params:
        key = param.name.strip()
        value = param.value.strip_code().strip()
        if value:
            data[key] = value
    return data

def count_pages_in_dump(dump_path):
    """
    ƒê·∫øm s·ªë l∆∞·ª£ng th·∫ª <page> trong file dump ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng ti·∫øn tr√¨nh.
    """
    count = 0
    with bz2.open(dump_path, 'rt', encoding='utf-8') as bz2f:
        for line in bz2f:
            if '<page>' in line:
                count += 1
    return count

def process_wikipedia_dump(dump_path, output_path):
    """
    ƒê·ªçc file dump c·ªßa Wikipedia, x·ª≠ l√Ω t·ª´ng b√†i vi·∫øt v√† l∆∞u
    th√¥ng tin c·∫ßn thi·∫øt (ti√™u ƒë·ªÅ, infobox, li√™n k·∫øt) v√†o file JSON Lines.
    """
    print("üîÑ ƒêang ƒë·∫øm t·ªïng s·ªë trang trong dump ƒë·ªÉ hi·ªÉn th·ªã ti·∫øn tr√¨nh (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    total_pages = count_pages_in_dump(dump_path)
    print(f"‚û°Ô∏è  ∆Ø·ªõc l∆∞·ª£ng kho·∫£ng {total_pages:,} trang c·∫ßn x·ª≠ l√Ω.\n")

    start_time = time.time()
    processed_with_infobox = 0

    with bz2.open(dump_path, 'rt', encoding='utf-8') as bz2f, \
         open(output_path, 'w', encoding='utf-8') as out_f, \
         tqdm(total=total_pages, desc="ƒêang x·ª≠ l√Ω", unit="trang") as pbar:

        context = ET.iterparse(bz2f, events=('end',))
        namespace = '{http://www.mediawiki.org/xml/export-0.10/}'
        
        for event, elem in context:
            if elem.tag == f'{namespace}page':
                title_elem = elem.find(f'{namespace}title')
                text_elem = elem.find(f'{namespace}revision/{namespace}text')
                
                if title_elem is None or text_elem is None or text_elem.text is None:
                    elem.clear()
                    pbar.update(1)
                    continue

                title = title_elem.text
                if ':' in title:  # b·ªè trang ƒë·∫∑c bi·ªát
                    elem.clear()
                    pbar.update(1)
                    continue

                wikicode = mwparserfromhell.parse(text_elem.text)
                infobox = extract_infobox_data(wikicode)

                if infobox:
                    links = [link.title.strip() for link in wikicode.filter_wikilinks()]
                    page_data = {
                        'title': title,
                        'infobox': infobox,
                        'links': links
                    }
                    out_f.write(json.dumps(page_data, ensure_ascii=False) + '\n')
                    processed_with_infobox += 1

                elem.clear()
                pbar.update(1)

    elapsed = time.time() - start_time
    print(f"\n‚úÖ Ho√†n th√†nh!")
    print(f"- T·ªïng s·ªë trang x·ª≠ l√Ω: {total_pages:,}")
    print(f"- S·ªë trang c√≥ Infobox: {processed_with_infobox:,}")
    print(f"- T·ª∑ l·ªá: {processed_with_infobox / total_pages * 100:.2f}%")
    print(f"- Th·ªùi gian ch·∫°y: {elapsed / 60:.2f} ph√∫t")
    print(f"- T·ªëc ƒë·ªô trung b√¨nh: {total_pages / elapsed:.2f} trang/gi√¢y")
    print(f"D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}")

if __name__ == '__main__':
    DUMP_FILE = '../../data/raw/viwiki-latest-pages-articles.xml.bz2'
    OUTPUT_FILE = '../../data/processed/viwiki_extracted.jsonl'

    if not os.path.exists(DUMP_FILE):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file dump t·∫°i '{DUMP_FILE}'.")
        print("Vui l√≤ng t·∫£i file v√† ƒë·∫∑t v√†o ƒë√∫ng th∆∞ m·ª•c.")
    else:
        process_wikipedia_dump(DUMP_FILE, OUTPUT_FILE)
