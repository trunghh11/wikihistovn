import requests
import json
import os
import time
import mwparserfromhell
import re
from typing import List, Dict, Set, Any, Tuple

# --- C·∫§U H√åNH ---
API_URL = "https://vi.wikipedia.org/w/api.php"
HEADERS = {
    "User-Agent": "VietnameseHistoryNetwork/1.0 (Project for university; contact: 22024527@vnu.edu.vn)"
}
# File input (t·ª´ script tr∆∞·ªõc)
SEED_FILE = '../../data/processed/seed_data_nguyen_kings_from_api.json'
# File output
OUTPUT_NODES = '../../data/processed/network_nodes_full.json'
OUTPUT_RELS = '../../data/processed/network_relationships_full.json'

# --- ƒê·ªäNH NGHƒ®A C√ÅC LO·∫†I C·∫†NH T·ª™ INFOBOX ---

# 1. C·∫°nh t·ª´ Infobox c·ªßa Vua (N·ªôi t·ªôc)
KING_REL_MAP = {
    # Key Infobox : (Lo·∫°i C·∫°nh, C·∫°nh ƒê·∫£o Ng∆∞·ª£c)
    'k·∫ø nhi·ªám': ('TI·ªÄN_NHI·ªÜM_C·ª¶A', 'K·∫æ_NHI·ªÜM_C·ª¶A'),
    'ti·ªÅn nhi·ªám': ('K·∫æ_NHI·ªÜM_C·ª¶A', 'TI·ªÄN_NHI·ªÜM_C·ª¶A'),
    'th√¢n ph·ª•': ('L√Ä_CON_C·ª¶A', 'L√Ä_CHA_C·ª¶A'),
    'cha': ('L√Ä_CON_C·ª¶A', 'L√Ä_CHA_C·ª¶A'),
    'th√¢n m·∫´u': ('L√Ä_CON_C·ª¶A', 'L√Ä_M·∫∏_C·ª¶A'),
    'm·∫π': ('L√Ä_CON_C·ª¶A', 'L√Ä_M·∫∏_C·ª¶A'),
    'ph·ªëi ng·∫´u': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'v·ª£': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'ch·ªìng': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'ch√≠nh th·∫•t': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'ph·ªëi m·∫´u': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'con c√°i': ('L√Ä_CHA_C·ª¶A', 'L√Ä_CON_C·ª¶A'),
    'h·∫≠u du·ªá': ('L√Ä_CHA_C·ª¶A', 'L√Ä_CON_C·ª¶A'),
    'anh em': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
    'anh': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
    'em': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
    'ch·ªã': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
    'em g√°i': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
    'em trai': ('L√Ä_ANH_EM_C·ª¶A', 'L√Ä_ANH_EM_C·ª¶A'),
}

# 2. C·∫°nh t·ª´ Infobox c·ªßa "H√†ng x√≥m" (Ngo·∫°i t·ªôc)
NEIGHBOR_REL_MAP = {
    'ph·ª•c v·ª•': ('PH·ª§C_V·ª§', 'ƒê∆Ø·ª¢C_PH·ª§C_V·ª§_B·ªûI'),
    'b·ªï nhi·ªám': ('ƒê∆Ø·ª¢C_B·ªî_NHI·ªÜM_B·ªûI', 'B·ªî_NHI·ªÜM'),
    'th·∫ßy gi√°o': ('L√Ä_TH·∫¶Y_C·ª¶A', 'L√Ä_TR√í_C·ª¶A'),
    'gi√°o vi√™n': ('L√Ä_TH·∫¶Y_C·ª¶A', 'L√Ä_TR√í_C·ª¶A'),
    'h·ªçc tr√≤': ('L√Ä_TR√í_C·ª¶A', 'L√Ä_TH·∫¶Y_C·ª¶A'),
    'ch·ªëng': ('CH·ªêNG_ƒê·ªêI', 'B·ªä_CH·ªêNG_ƒê·ªêI_B·ªûI'),
    'ƒë·ªëi th·ªß': ('ƒê·ªêI_TH·ª¶_C·ª¶A', 'ƒê·ªêI_TH·ª¶_C·ª¶A'),
    'tham chi·∫øn': ('THAM_GIA_S·ª∞_KI·ªÜN', 'C√ì_THAM_GIA_B·ªûI'),
    'tham gia': ('THAM_GIA_S·ª∞_KI·ªÜN', 'C√ì_THAM_GIA_B·ªûI'),
    'l√£nh ƒë·∫°o': ('L√ÉNH_ƒê·∫†O', 'ƒê∆Ø·ª¢C_L√ÉNH_ƒê·∫†O_B·ªûI'),
    'ch·ªâ huy': ('CH·ªà_HUY', 'ƒê∆Ø·ª¢C_CH·ªà_HUY_B·ªûI'),
    'ch·ªâ huy b·ªüi': ('ƒê∆Ø·ª¢C_CH·ªà_HUY_B·ªûI', 'CH·ªà_HUY'),
    'ch·ªâ huy 1': ('CH·ªà_HUY', 'ƒê∆Ø·ª¢C_CH·ªà_HUY_B·ªûI'),
    'ch·ªâ huy 2': ('CH·ªà_HUY', 'ƒê∆Ø·ª¢C_CH·ªà_HUY_B·ªûI'),
    'ƒë·ªìng minh': ('ƒê·ªíNG_MINH_V·ªöI', 'ƒê·ªíNG_MINH_V·ªöI'),
    'k·∫ª th√π': ('CH·ªêNG_ƒê·ªêI', 'B·ªä_CH·ªêNG_ƒê·ªêI_B·ªûI'),
    'ƒë·ªìng ƒë·ªôi': ('ƒê·ªíNG_ƒê·ªòI_V·ªöI', 'ƒê·ªíNG_ƒê·ªòI_V·ªöI'),
}

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---

def is_valid_link(title: str) -> bool:
    """L·ªçc c√°c li√™n k·∫øt chung chung, trang h·ªá th·ªëng, trang nƒÉm."""
    if not title: return False
    # L·ªçc c√°c trang c√≥ s·ªë
    if any(char.isdigit() for char in title): return False
    # L·ªçc ƒë·ªãa danh ph·ªï bi·∫øn
    if is_likely_location(title): return False
    # L·ªçc c√°c trang h·ªá th·ªëng
    if title.lower().startswith(('t·∫≠p tin:', 'th·ªÉ lo·∫°i:', 'b·∫£n m·∫´u:', 'danh s√°ch', 'wikipedia:', 'ch·ªß ƒë·ªÅ:')):
        return False
    # L·ªçc c√°c thu·∫≠t ng·ªØ chung
    common_terms = [
        'vi·ªát nam', 'ph√°p', 'nh√† nguy·ªÖn', 'l·ªãch s·ª≠ vi·ªát nam', 'ƒë·∫°i nam', 
        'ti·∫øng vi·ªát', 'h√°n t·ª±', 'qu·ªëc ng·ªØ', 'vi·ªát s·ª≠ th√¥ng gi√°m c∆∞∆°ng m·ª•c',
        'trung qu·ªëc', 'nh·∫≠t b·∫£n', 'ƒë√¥ng d∆∞∆°ng', 'ch√¢u √¢u', 'ch√¢u √°',
        'th·∫ø chi·∫øn', 'chi·∫øn tranh th·∫ø gi·ªõi', 'c√°ch m·∫°ng', 'ƒë·ªôc l·∫≠p',
        'vƒÉn h√≥a', 'x√£ h·ªôi', 'kinh t·∫ø', 'ch√≠nh tr·ªã', 't√¥n gi√°o',
        'ph·∫≠t gi√°o', 'nho gi√°o', 'ƒë·∫°o gi√°o', 'c√¥ng gi√°o', 'h·ªìi gi√°o',
        'ƒë·ªãa l√Ω', 'l·ªãch s·ª≠', 'nh√¢n v·∫≠t', 's·ª± ki·ªán', 'tri·ªÅu ƒë·∫°i',
        'ho√†ng ƒë·∫ø', 'vua', 'quan l·∫°i', 't∆∞·ªõng lƒ©nh', 'binh l√≠nh',
        'h·ªçc gi·∫£', 'nh√† th∆°', 'nh√† vƒÉn', 'nh√† s·ª≠ h·ªçc', 'nh√† khoa h·ªçc',
        'ƒë·ªãa danh', 's√¥ng', 'n√∫i', 'bi·ªÉn', 'ƒë·∫£o', 'th√†nh ph·ªë', 'l√†ng',
        'huy·ªán', 't·ªânh', 'qu·ªëc gia', 'ƒë·∫ø qu·ªëc', 'c·ªông h√≤a', 'v∆∞∆°ng qu·ªëc',
        'ch√≠nh ph·ªß', 'qu√¢n ƒë·ªôi', 'ƒë·∫£ng', 't·ªï ch·ª©c', 'h·ªôi ƒë·ªìng', '·ªßy ban'
    ]
    if title.lower() in common_terms: return False
    return True

def is_likely_location(title: str) -> bool:
    """
    Heuristic ph√°t hi·ªán ƒë·ªãa danh ƒë·ªÉ lo·∫°i b·ªè s·ªõm.
    Tr√°nh t·∫°o node ng∆∞·ªùi nh·∫ßm v·ªõi ƒë·ªãa ƒëi·ªÉm.
    """
    t = title.lower()
    # Kh·ªõp theo t·ª´ kh√≥a xu·∫•t hi·ªán trong chu·ªói
    location_keywords = [
        't·ªânh', 'th√†nh ph·ªë', 'qu·∫≠n', 'huy·ªán', 'x√£', 'ph∆∞·ªùng',
        'th·ªã x√£', 'th·ªã tr·∫•n', 'v·ªãnh', 'ƒë·∫£o', 'b√°n ƒë·∫£o', 'c·∫£ng',
        'c·∫ßu', 'ƒë√®o', 'ƒë·ªìi', 'n√∫i', 's√¥ng', 'su·ªëi', 'h·ªì', 'bi·ªÉn',
        'v∆∞·ªùn qu·ªëc gia', 'khu b·∫£o t·ªìn', 'c·ªë ƒë√¥', 'kinh th√†nh', 'th√†nh c·ªï',
        'lƒÉng', 'ƒë·ªÅn', 'ch√πa', 'mi·∫øu', 'th√°nh th·∫•t', 'nh√† th·ªù', 'tu vi·ªán'
    ]
    # Kh·ªõp theo h·∫≠u t·ªë ph·ªï bi·∫øn
    location_suffixes = [
        ' province', ' city', ' district', ' county', ' river', ' lake',
        ' bay', ' gulf', ' cape', ' strait', ' island', ' islands',
        ' archipelago', ' mountain', ' hill', ' range', ' bridge', ' port'
    ]
    # Bao quanh b·ªüi kho·∫£ng tr·∫Øng ƒë·ªÉ kh·ªõp t·ª´ kh√≥a nguy√™n v·∫πn
    wrapped = f" {t} "
    if any(f" {kw} " in wrapped for kw in location_keywords):
        return True
    if any(t.endswith(suf) for suf in location_suffixes):
        return True
    return False

def clean_infobox_value(value_wikitext: str) -> List[str]:
    """
    T√°ch c√°c gi√° tr·ªã trong m·ªôt tr∆∞·ªùng Infobox (v√≠ d·ª•: "A<br>B") th√†nh list [A, B].
    ƒê√¢y l√† h√†m quan tr·ªçng ƒë·ªÉ l√†m s·∫°ch t√™n th·ª±c th·ªÉ.
    """
    if not value_wikitext:
        return []
    
    # 1. T√°ch c√°c gi√° tr·ªã b·∫±ng <br> ho·∫∑c xu·ªëng d√≤ng
    items = re.split(r'<br\s*/?>|\n', str(value_wikitext))
    cleaned_items = []
    
    for item in items:
        # 2. D√πng mwparserfromhell ƒë·ªÉ l√†m s·∫°ch s√¢u (lo·∫°i b·ªè [[ ]], {{ }})
        parsed_item = mwparserfromhell.parse(item)
        
        # 3. L·∫•y vƒÉn b·∫£n thu·∫ßn t√∫y
        clean_text = parsed_item.strip_code().strip()
        
        # 4. Ch·ªâ l·∫•y ph·∫ßn tr∆∞·ªõc d·∫•u [ (ch√∫ th√≠ch) ho·∫∑c ( (ghi ch√∫)
        clean_text = re.split(r'\[|\(', clean_text, 1)[0].strip()
        
        if clean_text:
            cleaned_items.append(clean_text)
    return cleaned_items

def extract_infobox_data_and_label(wikicode: mwparserfromhell.wikicode) -> Tuple[Dict, str]:
    """
    Tr√≠ch xu·∫•t d·ªØ li·ªáu Infobox V√Ä x√°c ƒë·ªãnh Nh√£n (Label) cho Node.
    """
    infoboxes = wikicode.filter_templates(matches=lambda t: 'infobox' in t.name.lower() or 'th√¥ng tin' in t.name.lower())
    if not infoboxes: 
        return {}, 'Th·ª±c th·ªÉ' # Nh√£n m·∫∑c ƒë·ªãnh
    
    infobox = infoboxes[0]
    data = {}
    label = 'Th·ª±c th·ªÉ' # Nh√£n m·∫∑c ƒë·ªãnh
    
    # X√°c ƒë·ªãnh Nh√£n (Label)
    infobox_name = infobox.name.lower().strip()

    # T·ª´ kh√≥a li√™n quan ƒë·∫øn 'S·ª± ki·ªán'
    event_keywords = [
        'chi·∫øn tranh', 'tr·∫≠n ƒë√°nh', 'kh·ªüi nghƒ©a', 's·ª± ki·ªán', 'cu·ªôc chi·∫øn',
        'c√°ch m·∫°ng', 'phong tr√†o', 'xung ƒë·ªôt', 'n·ªïi d·∫≠y', 'th·∫£m s√°t',
        'h√≤a ∆∞·ªõc', 'hi·ªáp ƒë·ªãnh', 'tuy√™n ng√¥n', 'h·ªôi ngh·ªã', 'chi·∫øn d·ªãch',
        'th·ªùi k·ª≥', 'th·ªùi ƒë·∫°i', 'cu·ªôc n·ªïi d·∫≠y', 'cu·ªôc kh·ªüi nghƒ©a', 'cu·ªôc chi·∫øn tranh',
        'tr·∫≠n chi·∫øn', 'tr·∫≠n ƒë√°nh', 'th·∫£m h·ªça', 'cu·ªôc di c∆∞', 'cu·ªôc di·ªát ch·ªßng'
    ]
    
    # T·ª´ kh√≥a li√™n quan ƒë·∫øn 'Nh√¢n v·∫≠t L·ªãch s·ª≠'
    person_keywords = [
        'nh√¢n v·∫≠t', 'quan l·∫°i', 't∆∞·ªõng', 'l√£nh ƒë·∫°o',
        'nh√† th∆°', 'nh√† vƒÉn', 'nh√† s·ª≠ h·ªçc', 'nh√† khoa h·ªçc', 'nh√† gi√°o',
        'ch√≠nh tr·ªã gia', 'nh√† ho·∫°t ƒë·ªông', 'nh√† c√°ch m·∫°ng', 'ho√†ng h·∫≠u',
        'c√¥ng ch√∫a', 'ho√†ng t·ª≠', 'nh√† ngo·∫°i giao', 'nh√† qu√¢n s·ª±',
        'nh√† t∆∞ t∆∞·ªüng', 'nh√† tri·∫øt h·ªçc', 'nh√† c·∫£i c√°ch', 'nh√† truy·ªÅn gi√°o',
        'nh√† s√°ng l·∫≠p', 'nh√† cai tr·ªã', 'nh√† l√£nh ƒë·∫°o', 'nh√† th√°m hi·ªÉm'
    ]
    
    # Ki·ªÉm tra t·ª´ kh√≥a ƒë·ªÉ x√°c ƒë·ªãnh nh√£n
    if any(keyword in infobox_name for keyword in event_keywords):
        label = 'S·ª± ki·ªán'
    elif any(keyword in infobox_name for keyword in person_keywords):
        label = 'Nh√¢n v·∫≠t L·ªãch s·ª≠'
    
    # Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ Infobox
    for param in infobox.params:
        key = param.name.strip()
        value = param.value  # Gi·ªØ nguy√™n wikitext ƒë·ªÉ h√†m clean_infobox_value x·ª≠ l√Ω
        if value:
            data[key] = str(value)
    return data, label

def fetch_data_by_titles(titles_list: List[str]) -> Dict[str, Dict]:
    """
    G·ªçi API ƒë·ªÉ "l√†m gi√†u" d·ªØ li·ªáu, x·ª≠ l√Ω theo batch 50
    V√Ä B√ÅO C√ÅO TI·∫æN TR√åNH. N·∫øu c√≥ node b·ªã r·ªóng infobox, fetch l·∫°i b·∫±ng page_id.
    """
    total_items = len(titles_list)
    total_batches = (total_items // 50) + (1 if total_items % 50 > 0 else 0)
    
    print(f"  > ƒêang g·ªçi API ƒë·ªÉ l√†m gi√†u {total_items} h√†ng x√≥m (chia th√†nh {total_batches} l∆∞·ª£t g·ªçi)...")
    enriched_data = {}
    missing_pages = []  # Danh s√°ch c√°c page_id c·∫ßn fetch l·∫°i

    # --- FETCH B·∫∞NG TITLES ---
    for i in range(0, total_items, 50):
        current_batch_num = (i // 50) + 1
        batch_titles = titles_list[i:i+50]
        
        params = {
            "action": "query", "format": "json", "titles": "|".join(batch_titles),
            "prop": "revisions|info", "rvprop": "content", "formatversion": "2"
        }
        
        print(f"    > ƒêang x·ª≠ l√Ω l∆∞·ª£t {current_batch_num}/{total_batches} ({len(batch_titles)} m·ª•c)...")
        
        try:
            time.sleep(1)  # L·ªãch s·ª± v·ªõi API
            response = requests.get(API_URL, params=params, headers=HEADERS)
            response.raise_for_status()
            pages = response.json().get("query", {}).get("pages", [])
            
            for page in pages:
                if "missing" in page:
                    continue
                
                title = page['title']
                wikitext = page.get("revisions", [{}])[0].get("content", "")
                if not wikitext:
                    print(f"    ‚ö†Ô∏è Trang '{title}' kh√¥ng c√≥ n·ªôi dung, th√™m v√†o danh s√°ch fetch l·∫°i.")
                    missing_pages.append(page['pageid'])
                    continue
                
                wikicode = mwparserfromhell.parse(wikitext)
                infobox, label = extract_infobox_data_and_label(wikicode)
                links = [link.title.strip() for link in wikicode.filter_wikilinks()]
                
                enriched_data[title] = {
                    'page_id': page['pageid'], 'title': title,
                    'infobox': infobox, 'links': links, 'label': label
                }
        except Exception as e:
            print(f"    ! L·ªói khi fetch l∆∞·ª£t {current_batch_num}: {e}")
    
    # --- FETCH L·∫†I B·∫∞NG PAGE ID ---
    if missing_pages:
        print(f"\n  > ƒêang fetch l·∫°i {len(missing_pages)} trang b·ªã thi·∫øu b·∫±ng page_id...")
        for i in range(0, len(missing_pages), 50):
            current_batch_num = (i // 50) + 1
            batch_ids = missing_pages[i:i+50]
            
            params = {
                "action": "query", "format": "json", "pageids": "|".join(map(str, batch_ids)),
                "prop": "revisions|info", "rvprop": "content", "formatversion": "2"
            }
            
            print(f"    > ƒêang x·ª≠ l√Ω l∆∞·ª£t {current_batch_num}/{len(missing_pages)//50 + 1} ({len(batch_ids)} m·ª•c)...")
            
            try:
                time.sleep(1)  # L·ªãch s·ª± v·ªõi API
                response = requests.get(API_URL, params=params, headers=HEADERS)
                response.raise_for_status()
                pages = response.json().get("query", {}).get("pages", [])
                
                for page in pages:
                    if "missing" in page:
                        continue
                    
                    title = page['title']
                    wikitext = page.get("revisions", [{}])[0].get("content", "")
                    if not wikitext:
                        print(f"    ‚ö†Ô∏è Trang '{title}' v·∫´n kh√¥ng c√≥ n·ªôi dung sau khi fetch l·∫°i.")
                        continue
                    
                    wikicode = mwparserfromhell.parse(wikitext)
                    infobox, label = extract_infobox_data_and_label(wikicode)
                    links = [link.title.strip() for link in wikicode.filter_wikilinks()]
                    
                    enriched_data[title] = {
                        'page_id': page['pageid'], 'title': title,
                        'infobox': infobox, 'links': links, 'label': label
                    }
            except Exception as e:
                print(f"    ! L·ªói khi fetch l·∫°i b·∫±ng page_id: {e}")
    
    print(f"  > L√†m gi√†u th√†nh c√¥ng {len(enriched_data)}/{total_items} h√†ng x√≥m.")
    return enriched_data

def add_relationship(source: str, target: str, type: str, all_rels: List, rel_set: Set):
    if type in ('PH·ªêI_NG·∫™U_V·ªöI', 'ƒê·ªêI_TH·ª¶_C·ª¶A'):
        key = tuple(sorted((source, target))) + (type,)
    else:
        key = (source, target, type)
        
    if key not in rel_set:
        rel_set.add(key)
        all_rels.append({'source': source, 'target': target, 'type': type})

# --- H√ÄM CH√çNH ƒêI·ªÄU PH·ªêI QU√Å TR√åNH ---

def build_full_network():
    print("--- üöÄ B·∫Øt ƒë·∫ßu x√¢y d·ª±ng m·∫°ng l∆∞·ªõi (2 L·ªõp) ---")
    
    # --- KH·ªûI T·∫†O ---
    all_nodes: Dict[str, Dict] = {}        # Dict ch·ª©a t·∫•t c·∫£ c√°c node, key l√† title
    all_relationships: List[Dict] = []   # List ch·ª©a t·∫•t c·∫£ c√°c c·∫°nh
    rel_set: Set[tuple] = set()          # Set ƒë·ªÉ ki·ªÉm tra tr√πng l·∫∑p c·∫°nh
    neighbors_to_fetch: Set[str] = set() # Set ch·ª©a c√°c h√†ng x√≥m c·∫ßn "l√†m gi√†u"
    
    try:
        with open(SEED_FILE, 'r', encoding='utf-8') as f:
            seed_data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file h·∫°t gi·ªëng '{SEED_FILE}'. D·ª´ng l·∫°i.")
        return

    # --- L·ªöP 1 (PASS 1): X·ª¨ L√ù H·∫†T GI·ªêNG (13 VUA) ---
    print(f"--- L·ªõp 1: ƒêang x·ª≠ l√Ω {len(seed_data)} Node H·∫°t Gi·ªëng (Vua) ---")
    for king in seed_data:
        title = king['title']
        
        # 1. Th√™m Node Vua
        all_nodes[title] = {
            'page_id': king['page_id'], 'title': title,
            'label': 'Vua Nh√† Nguy·ªÖn', 'infobox': king.get('infobox', {})
        }
        
        # 2. T·∫°o C·∫°nh N·ªôi T·ªôc (Gia ƒë√¨nh, K·∫ø v·ªã)
        infobox = king.get('infobox', {})
        for key, (rel_type, rev_type) in KING_REL_MAP.items():
            if key in infobox:
                targets = clean_infobox_value(infobox[key])
                for target_title in targets:
                    add_relationship(title, target_title, rel_type, all_relationships, rel_set)

        # 3. Thu th·∫≠p "H√†ng x√≥m" ƒë·ªÉ l√†m gi√†u ·ªü L·ªõp 2
        for link_title in king.get('links', []):
            if is_valid_link(link_title):
                neighbors_to_fetch.add(link_title)

    print(f"  > X·ª≠ l√Ω xong {len(all_nodes)} vua.")
    print(f"  > T√¨m th·∫•y {len(neighbors_to_fetch)} h√†ng x√≥m ti·ªÅm nƒÉng ƒë·ªÉ l√†m gi√†u.")

    # --- L·ªöP 2 (PASS 2): L√ÄM GI√ÄU "H√ÄNG X√ìM" ---
    print("\n--- L·ªõp 2: ƒêang l√†m gi√†u H√†ng x√≥m (Quan l·∫°i, K·∫ª th√π, S·ª± ki·ªán...) ---")
    
    # Lo·∫°i b·ªè c√°c vua ra kh·ªèi danh s√°ch fetch (v√¨ ƒë√£ c√≥ th√¥ng tin)
    neighbors_to_fetch.difference_update(all_nodes.keys())
    enriched_neighbors = fetch_data_by_titles(list(neighbors_to_fetch))
    
    for neighbor_title, neighbor_data in enriched_neighbors.items():
        # 1. Th√™m Node H√†ng x√≥m (Quan l·∫°i, K·∫ª th√π,...)
        if neighbor_title not in all_nodes:
            all_nodes[neighbor_title] = {
                'page_id': neighbor_data['page_id'], 'title': neighbor_title,
                'label': neighbor_data['label'], # S·ª≠ d·ª•ng nh√£n ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i
                'infobox': neighbor_data.get('infobox', {})
            }
        
        # 2. T·∫°o C·∫°nh Ngo·∫°i T·ªôc (Ph·ª•c v·ª•, Ch·ªëng ƒë·ªëi, ...)
        infobox = neighbor_data.get('infobox', {})
        for key, (rel_type, rev_type) in NEIGHBOR_REL_MAP.items():
            if key in infobox:
                targets = clean_infobox_value(infobox[key])
                for target_title in targets:
                    # Ch·ªâ t·∫°o c·∫°nh n·∫øu n√≥ n·ªëi v·ªõi m·ªôt node ta ƒë√£ bi·∫øt (v√≠ d·ª•: Vua)
                    if target_title in all_nodes:
                        add_relationship(neighbor_title, target_title, rel_type, all_relationships, rel_set)
    
    print(f"  > ƒê√£ th√™m {len(enriched_neighbors)} node h√†ng x√≥m v√†o m·∫°ng l∆∞·ªõi.")

    # --- L·ªöP 3 (PASS 3): T·∫†O C·∫†NH "FALLBACK" LI√äN_K·∫æT_T·ªöI ---
    print("\n--- L·ªõp 3: ƒêang t·∫°o c√°c C·∫°nh 'LI√äN_K·∫æT_T·ªöI' (Fallback) ---")
    
    # Ch·ªâ t·∫°o fallback t·ª´ c√°c Vua (H·∫°t gi·ªëng)
    for king in seed_data:
        source_title = king['title']
        for target_title in king.get('links', []):
            if not is_valid_link(target_title):
                continue
            
            # Ch·ªâ t·∫°o n·∫øu node ƒë√≠ch ƒë√£ n·∫±m trong m·∫°ng l∆∞·ªõi c·ªßa ch√∫ng ta
            if target_title not in all_nodes:
                continue
            
            # Ki·ªÉm tra xem ƒë√£ c√≥ c·∫°nh n√†o (b·∫•t k·ª≥ chi·ªÅu) gi·ªØa 2 node n√†y ch∆∞a
            rel_exists = any(
                (r['source'] == source_title and r['target'] == target_title) or \
                (r['source'] == target_title and r['target'] == source_title) \
                for r in all_relationships
            )
            
            if not rel_exists:
                add_relationship(source_title, target_title, 'LI√äN_K·∫æT_T·ªöI', all_relationships, rel_set)
    
    print(f"  > ƒê√£ t·∫°o xong c√°c c·∫°nh fallback.")

    # --- L∆ØU K·∫æT QU·∫¢ ---
    print("\n--- HO√ÄN T·∫§T ---")
    
    final_nodes = list(all_nodes.values())
    
    with open(OUTPUT_NODES, 'w', encoding='utf-8') as f:
        json.dump(final_nodes, f, indent=4, ensure_ascii=False)
        
    with open(OUTPUT_RELS, 'w', encoding='utf-8') as f:
        json.dump(all_relationships, f, indent=4, ensure_ascii=False)

    print(f"üìä T·ªïng s·ªë Nodes: {len(final_nodes)}")
    print(f"‚ÜîÔ∏è T·ªïng s·ªë Relationships (C·∫°nh): {len(all_relationships)}")
    print(f"‚úÖ ƒê√£ l∆∞u file v√†o '{OUTPUT_NODES}' v√† '{OUTPUT_RELS}'")

# --- CH·∫†Y SCRIPT ---
if __name__ == "__main__":
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c processed t·ªìn t·∫°i
    os.makedirs(os.path.dirname(OUTPUT_NODES), exist_ok=True)
    
    build_full_network()