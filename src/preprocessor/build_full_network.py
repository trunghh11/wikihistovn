import requests
import json
import os
import time
import mwparserfromhell
import re
from typing import List, Dict, Set, Any, Tuple

# --- C·∫§U H√åNH ---
API_URL = "https://vi.wikipedia.org/w/api.php"
HEADERS = {
    "User-Agent": "VietnameseHistoryNetwork/1.0 (Project for university; contact: 22024527@vnu.edu.vn)"
}
# File input (t·ª´ script tr∆∞·ªõc)
SEED_FILE = '../../data/processed/seed_data_nguyen_kings_from_api.json'
# File output
OUTPUT_NODES = '../../data/processed/network_nodes_full.json'
OUTPUT_RELS = '../../data/processed/network_relationships_full.json'

# --- ƒê·ªäNH NGHƒ®A C√ÅC LO·∫†I C·∫†NH T·ª™ INFOBOX ---

# 1. C·∫°nh t·ª´ Infobox c·ªßa Vua (N·ªôi t·ªôc)
KING_REL_MAP = {
    # Key Infobox : (Lo·∫°i C·∫°nh, C·∫°nh ƒê·∫£o Ng∆∞·ª£c)
    'k·∫ø nhi·ªám': ('TI·ªÄN_NHI·ªÜM_C·ª¶A', 'K·∫æ_NHI·ªÜM_C·ª¶A'),
    'ti·ªÅn nhi·ªám': ('K·∫æ_NHI·ªÜM_C·ª¶A', 'TI·ªÄN_NHI·ªÜM_C·ª¶A'),
    'th√¢n ph·ª•': ('L√Ä_CON_C·ª¶A', 'L√Ä_CHA_C·ª¶A'),
    'th√¢n m·∫´u': ('L√Ä_CON_C·ª¶A', 'L√Ä_M·∫∏_C·ª¶A'),
    'ph·ªëi ng·∫´u': ('PH·ªêI_NG·∫™U_V·ªöI', 'PH·ªêI_NG·∫™U_V·ªöI'),
    'con c√°i': ('L√Ä_CHA_C·ª¶A', 'L√Ä_CON_C·ª¶A') # Gi·∫£ ƒë·ªãnh Vua l√† cha
}

# 2. C·∫°nh t·ª´ Infobox c·ªßa "H√†ng x√≥m" (Ngo·∫°i t·ªôc)
NEIGHBOR_REL_MAP = {
    'ph·ª•c v·ª•': ('PH·ª§C_V·ª§', 'ƒê∆Ø·ª¢C_PH·ª§C_V·ª§_B·ªûI'),
    'b·ªï nhi·ªám': ('ƒê∆Ø·ª¢C_B·ªî_NHI·ªÜM_B·ªûI', 'B·ªî_NHI·ªÜM'),
    'th·∫ßy gi√°o': ('L√Ä_TH·∫¶Y_C·ª¶A', 'L√Ä_TR√í_C·ª¶A'),
    'h·ªçc tr√≤': ('L√Ä_TR√í_C·ª¶A', 'L√Ä_TH·∫¶Y_C·ª¶A'),
    'ch·ªëng': ('CH·ªêNG_ƒê·ªêI', 'B·ªä_CH·ªêNG_ƒê·ªêI_B·ªûI'),
    'ƒë·ªëi th·ªß': ('ƒê·ªêI_TH·ª¶_C·ª¶A', 'ƒê·ªêI_TH·ª¶_C·ª¶A'),
    'tham chi·∫øn': ('THAM_GIA_S·ª∞_KI·ªÜN', 'C√ì_THAM_GIA_B·ªûI'),
    'l√£nh ƒë·∫°o': ('L√ÉNH_ƒê·∫†O', 'ƒê∆Ø·ª¢C_L√ÉNH_ƒê·∫†O_B·ªûI'),
    'ch·ªâ huy': ('CH·ªà_HUY', 'ƒê∆Ø·ª¢C_CH·ªà_HUY_B·ªûI')
}

# --- C√ÅC H√ÄM TI·ªÜN √çCH ---

def is_valid_link(title: str) -> bool:
    """L·ªçc c√°c li√™n k·∫øt chung chung, trang h·ªá th·ªëng, trang nƒÉm."""
    if not title: return False
    # L·ªçc c√°c trang c√≥ s·ªë
    if any(char.isdigit() for char in title): return False
    # L·ªçc c√°c trang h·ªá th·ªëng
    if title.lower().startswith(('t·∫≠p tin:', 'th·ªÉ lo·∫°i:', 'b·∫£n m·∫´u:', 'danh s√°ch', 'wikipedia:', 'ch·ªß ƒë·ªÅ:')):
        return False
    # L·ªçc c√°c thu·∫≠t ng·ªØ chung
    common_terms = [
        'vi·ªát nam', 'ph√°p', 'nh√† nguy·ªÖn', 'l·ªãch s·ª≠ vi·ªát nam', 'ƒë·∫°i nam', 
        'ti·∫øng vi·ªát', 'h√°n t·ª±', 'qu·ªëc ng·ªØ', 'vi·ªát s·ª≠ th√¥ng gi√°m c∆∞∆°ng m·ª•c'
    ]
    if title.lower() in common_terms: return False
    return True

def clean_infobox_value(value_wikitext: str) -> List[str]:
    """
    T√°ch c√°c gi√° tr·ªã trong m·ªôt tr∆∞·ªùng Infobox (v√≠ d·ª•: "A<br>B") th√†nh list [A, B].
    ƒê√¢y l√† h√†m quan tr·ªçng ƒë·ªÉ l√†m s·∫°ch t√™n th·ª±c th·ªÉ.
    """
    if not value_wikitext:
        return []
    
    # 1. T√°ch c√°c gi√° tr·ªã b·∫±ng <br> ho·∫∑c xu·ªëng d√≤ng
    items = re.split(r'<br\s*/?>|\n', str(value_wikitext))
    cleaned_items = []
    
    for item in items:
        # 2. D√πng mwparserfromhell ƒë·ªÉ l√†m s·∫°ch s√¢u (lo·∫°i b·ªè [[ ]], {{ }})
        parsed_item = mwparserfromhell.parse(item)
        
        # 3. L·∫•y vƒÉn b·∫£n thu·∫ßn t√∫y
        clean_text = parsed_item.strip_code().strip()
        
        # 4. Ch·ªâ l·∫•y ph·∫ßn tr∆∞·ªõc d·∫•u [ (ch√∫ th√≠ch) ho·∫∑c ( (ghi ch√∫)
        clean_text = re.split(r'\[|\(', clean_text, 1)[0].strip()
        
        if clean_text:
            cleaned_items.append(clean_text)
    return cleaned_items

def extract_infobox_data_and_label(wikicode: mwparserfromhell.wikicode) -> Tuple[Dict, str]:
    """
    Tr√≠ch xu·∫•t d·ªØ li·ªáu Infobox V√Ä x√°c ƒë·ªãnh Nh√£n (Label) cho Node.
    """
    infoboxes = wikicode.filter_templates(matches=lambda t: 'infobox' in t.name.lower() or 'th√¥ng tin' in t.name.lower())
    if not infoboxes: 
        return {}, 'Th·ª±c th·ªÉ' # Nh√£n m·∫∑c ƒë·ªãnh
    
    infobox = infoboxes[0]
    data = {}
    label = 'Th·ª±c th·ªÉ' # Nh√£n m·∫∑c ƒë·ªãnh
    
    # X√°c ƒë·ªãnh Nh√£n (Label)
    infobox_name = infobox.name.lower().strip()
    if 'chi·∫øn tranh' in infobox_name or 'tr·∫≠n ƒë√°nh' in infobox_name or 'kh·ªüi nghƒ©a' in infobox_name:
        label = 'S·ª± ki·ªán'
    elif 'quan l·∫°i' in infobox_name or 'ho√†ng t·ªôc' in infobox_name or 'nh√¢n v·∫≠t' in infobox_name:
        label = 'Nh√¢n v·∫≠t L·ªãch s·ª≠'
    
    # Tr√≠ch xu·∫•t d·ªØ li·ªáu
    for param in infobox.params:
        key = param.name.strip()
        value = param.value # Gi·ªØ nguy√™n wikitext ƒë·ªÉ h√†m clean_infobox_value x·ª≠ l√Ω
        if value:
            data[key] = str(value) # Chuy·ªÉn th√†nh string
    return data, label

def fetch_data_by_titles(titles_list: List[str]) -> Dict[str, Dict]:
    """
    G·ªçi API ƒë·ªÉ "l√†m gi√†u" d·ªØ li·ªáu, x·ª≠ l√Ω theo batch 50
    V√Ä B√ÅO C√ÅO TI·∫æN TR√åNH.
    """
    total_items = len(titles_list)
    # T√≠nh to√°n ch√≠nh x√°c t·ªïng s·ªë l∆∞·ª£t g·ªçi
    total_batches = (total_items // 50) + (1 if total_items % 50 > 0 else 0)
    
    print(f"  > ƒêang g·ªçi API ƒë·ªÉ l√†m gi√†u {total_items} h√†ng x√≥m (chia th√†nh {total_batches} l∆∞·ª£t g·ªçi)...")
    enriched_data = {}
    
    for i in range(0, total_items, 50):
        current_batch_num = (i // 50) + 1
        batch_titles = titles_list[i:i+50]
        
        params = {
            "action": "query", "format": "json", "titles": "|".join(batch_titles),
            "prop": "revisions|info", "rvprop": "content", "formatversion": "2"
        }
        
        # --- B√ÅO C√ÅO TI·∫æN TR√åNH ---
        print(f"    > ƒêang x·ª≠ l√Ω l∆∞·ª£t {current_batch_num}/{total_batches} ({len(batch_titles)} m·ª•c)...")
        
        try:
            time.sleep(1) # L·ªãch s·ª± v·ªõi API
            response = requests.get(API_URL, params=params, headers=HEADERS)
            response.raise_for_status()
            pages = response.json().get("query", {}).get("pages", [])
            
            for page in pages:
                if "missing" in page:
                    continue
                
                title = page['title']
                wikitext = page.get("revisions", [{}])[0].get("content", "")
                wikicode = mwparserfromhell.parse(wikitext)
                
                infobox, label = extract_infobox_data_and_label(wikicode)
                links = [link.title.strip() for link in wikicode.filter_wikilinks()]
                
                enriched_data[title] = {
                    'page_id': page['pageid'], 'title': title,
                    'infobox': infobox, 'links': links, 'label': label
                }
        except Exception as e:
            # --- B√ÅO C√ÅO L·ªñI C·ª§ TH·ªÇ ---
            print(f"    ! L·ªói khi fetch l∆∞·ª£t {current_batch_num}: {e}")
            
    print(f"  > L√†m gi√†u th√†nh c√¥ng {len(enriched_data)}/{total_items} h√†ng x√≥m.")
    return enriched_data

def add_relationship(source: str, target: str, type: str, all_rels: List, rel_set: Set):
    if type in ('PH·ªêI_NG·∫™U_V·ªöI', 'ƒê·ªêI_TH·ª¶_C·ª¶A'):
        key = tuple(sorted((source, target))) + (type,)
    else:
        key = (source, target, type)
        
    if key not in rel_set:
        rel_set.add(key)
        all_rels.append({'source': source, 'target': target, 'type': type})

# --- H√ÄM CH√çNH ƒêI·ªÄU PH·ªêI QU√Å TR√åNH ---

def build_full_network():
    print("--- üöÄ B·∫Øt ƒë·∫ßu x√¢y d·ª±ng m·∫°ng l∆∞·ªõi (2 L·ªõp) ---")
    
    # --- KH·ªûI T·∫†O ---
    all_nodes: Dict[str, Dict] = {}        # Dict ch·ª©a t·∫•t c·∫£ c√°c node, key l√† title
    all_relationships: List[Dict] = []   # List ch·ª©a t·∫•t c·∫£ c√°c c·∫°nh
    rel_set: Set[tuple] = set()          # Set ƒë·ªÉ ki·ªÉm tra tr√πng l·∫∑p c·∫°nh
    neighbors_to_fetch: Set[str] = set() # Set ch·ª©a c√°c h√†ng x√≥m c·∫ßn "l√†m gi√†u"
    
    try:
        with open(SEED_FILE, 'r', encoding='utf-8') as f:
            seed_data = json.load(f)
    except FileNotFoundError:
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file h·∫°t gi·ªëng '{SEED_FILE}'. D·ª´ng l·∫°i.")
        return

    # --- L·ªöP 1 (PASS 1): X·ª¨ L√ù H·∫†T GI·ªêNG (13 VUA) ---
    print(f"--- L·ªõp 1: ƒêang x·ª≠ l√Ω {len(seed_data)} Node H·∫°t Gi·ªëng (Vua) ---")
    for king in seed_data:
        title = king['title']
        
        # 1. Th√™m Node Vua
        all_nodes[title] = {
            'page_id': king['page_id'], 'title': title,
            'label': 'Vua Nh√† Nguy·ªÖn', 'infobox': king.get('infobox', {})
        }
        
        # 2. T·∫°o C·∫°nh N·ªôi T·ªôc (Gia ƒë√¨nh, K·∫ø v·ªã)
        infobox = king.get('infobox', {})
        for key, (rel_type, rev_type) in KING_REL_MAP.items():
            if key in infobox:
                targets = clean_infobox_value(infobox[key])
                for target_title in targets:
                    add_relationship(title, target_title, rel_type, all_relationships, rel_set)

        # 3. Thu th·∫≠p "H√†ng x√≥m" ƒë·ªÉ l√†m gi√†u ·ªü L·ªõp 2
        for link_title in king.get('links', []):
            if is_valid_link(link_title):
                neighbors_to_fetch.add(link_title)

    print(f"  > X·ª≠ l√Ω xong {len(all_nodes)} vua.")
    print(f"  > T√¨m th·∫•y {len(neighbors_to_fetch)} h√†ng x√≥m ti·ªÅm nƒÉng ƒë·ªÉ l√†m gi√†u.")

    # --- L·ªöP 2 (PASS 2): L√ÄM GI√ÄU "H√ÄNG X√ìM" ---
    print("\n--- L·ªõp 2: ƒêang l√†m gi√†u H√†ng x√≥m (Quan l·∫°i, K·∫ª th√π, S·ª± ki·ªán...) ---")
    
    # Lo·∫°i b·ªè c√°c vua ra kh·ªèi danh s√°ch fetch (v√¨ ƒë√£ c√≥ th√¥ng tin)
    neighbors_to_fetch.difference_update(all_nodes.keys())
    enriched_neighbors = fetch_data_by_titles(list(neighbors_to_fetch))
    
    for neighbor_title, neighbor_data in enriched_neighbors.items():
        # 1. Th√™m Node H√†ng x√≥m (Quan l·∫°i, K·∫ª th√π,...)
        if neighbor_title not in all_nodes:
            all_nodes[neighbor_title] = {
                'page_id': neighbor_data['page_id'], 'title': neighbor_title,
                'label': neighbor_data['label'], # S·ª≠ d·ª•ng nh√£n ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i
                'infobox': neighbor_data.get('infobox', {})
            }
        
        # 2. T·∫°o C·∫°nh Ngo·∫°i T·ªôc (Ph·ª•c v·ª•, Ch·ªëng ƒë·ªëi, ...)
        infobox = neighbor_data.get('infobox', {})
        for key, (rel_type, rev_type) in NEIGHBOR_REL_MAP.items():
            if key in infobox:
                targets = clean_infobox_value(infobox[key])
                for target_title in targets:
                    # Ch·ªâ t·∫°o c·∫°nh n·∫øu n√≥ n·ªëi v·ªõi m·ªôt node ta ƒë√£ bi·∫øt (v√≠ d·ª•: Vua)
                    if target_title in all_nodes:
                        add_relationship(neighbor_title, target_title, rel_type, all_relationships, rel_set)
    
    print(f"  > ƒê√£ th√™m {len(enriched_neighbors)} node h√†ng x√≥m v√†o m·∫°ng l∆∞·ªõi.")

    # --- L·ªöP 3 (PASS 3): T·∫†O C·∫†NH "FALLBACK" LI√äN_K·∫æT_T·ªöI ---
    print("\n--- L·ªõp 3: ƒêang t·∫°o c√°c C·∫°nh 'LI√äN_K·∫æT_T·ªöI' (Fallback) ---")
    
    # Ch·ªâ t·∫°o fallback t·ª´ c√°c Vua (H·∫°t gi·ªëng)
    for king in seed_data:
        source_title = king['title']
        for target_title in king.get('links', []):
            if not is_valid_link(target_title):
                continue
            
            # Ch·ªâ t·∫°o n·∫øu node ƒë√≠ch ƒë√£ n·∫±m trong m·∫°ng l∆∞·ªõi c·ªßa ch√∫ng ta
            if target_title not in all_nodes:
                continue
            
            # Ki·ªÉm tra xem ƒë√£ c√≥ c·∫°nh n√†o (b·∫•t k·ª≥ chi·ªÅu) gi·ªØa 2 node n√†y ch∆∞a
            rel_exists = any(
                (r['source'] == source_title and r['target'] == target_title) or \
                (r['source'] == target_title and r['target'] == source_title) \
                for r in all_relationships
            )
            
            if not rel_exists:
                add_relationship(source_title, target_title, 'LI√äN_K·∫æT_T·ªöI', all_relationships, rel_set)
    
    print(f"  > ƒê√£ t·∫°o xong c√°c c·∫°nh fallback.")

    # --- L∆ØU K·∫æT QU·∫¢ ---
    print("\n--- HO√ÄN T·∫§T ---")
    
    final_nodes = list(all_nodes.values())
    
    with open(OUTPUT_NODES, 'w', encoding='utf-8') as f:
        json.dump(final_nodes, f, indent=4, ensure_ascii=False)
        
    with open(OUTPUT_RELS, 'w', encoding='utf-8') as f:
        json.dump(all_relationships, f, indent=4, ensure_ascii=False)

    print(f"üìä T·ªïng s·ªë Nodes: {len(final_nodes)}")
    print(f"‚ÜîÔ∏è T·ªïng s·ªë Relationships (C·∫°nh): {len(all_relationships)}")
    print(f"‚úÖ ƒê√£ l∆∞u file v√†o '{OUTPUT_NODES}' v√† '{OUTPUT_RELS}'")

# --- CH·∫†Y SCRIPT ---
if __name__ == "__main__":
    # ƒê·∫£m b·∫£o th∆∞ m·ª•c processed t·ªìn t·∫°i
    os.makedirs(os.path.dirname(OUTPUT_NODES), exist_ok=True)
    
    build_full_network()