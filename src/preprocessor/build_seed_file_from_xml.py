import xml.etree.cElementTree as ET
import mwparserfromhell
import json
import os

# --- ƒê√É C·∫¨P NH·∫¨T THEO Y√äU C·∫¶U ---
# T·∫≠p h·∫°t gi·ªëng: Dictionary ch·ª©a t√™n v√† Page ID c·ªßa 13 v·ªã vua tri·ªÅu Nguy·ªÖn.
NGUYEN_KINGS_PAGES = {
    11680: "Gia Long",
    11667: "Minh M·∫°ng",
    41716: "Thi·ªáu Tr·ªã",
    41279: "T·ª± ƒê·ª©c",
    41729: "D·ª•c ƒê·ª©c",
    357286: "Hi·ªáp H√≤a",
    41740: "Ki·∫øn Ph√∫c",
    41820: "H√†m Nghi",
    41744: "ƒê·ªìng Kh√°nh",
    41630: "Th√†nh Th√°i",
    41862: "Duy T√¢n",
    42008: "Kh·∫£i ƒê·ªãnh",
    15247: "B·∫£o ƒê·∫°i"
}

def extract_infobox_data(wikicode):
    """Tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ template Infobox ƒë·∫ßu ti√™n t√¨m th·∫•y."""
    infoboxes = wikicode.filter_templates(matches=lambda t: 'infobox' in t.name.lower())
    if not infoboxes:
        return None
    
    infobox = infoboxes[0]
    data = {}
    for param in infobox.params:
        key = param.name.strip()
        value = param.value.strip_code().strip()
        if value:
            data[key] = value
    return data

def create_seed_list_from_xml(dump_path, seed_ids, output_path):
    """
    Qu√©t file XML, t√¨m c√°c b√†i vi·∫øt trong t·∫≠p h·∫°t gi·ªëng b·∫±ng Page ID,
    v√† l∆∞u k·∫øt qu·∫£ v√†o m·ªôt file JSON duy nh·∫•t.
    """
    print(f"üëë B·∫Øt ƒë·∫ßu qu√©t file XML ƒë·ªÉ x√¢y d·ª±ng t·∫≠p h·∫°t gi·ªëng (phi√™n b·∫£n ID ƒë√£ c·∫≠p nh·∫≠t)...")
    
    # D√πng set ƒë·ªÉ t√¨m ki·∫øm ID nhanh h∆°n
    target_ids = set(seed_ids.keys())
    
    # N∆°i l∆∞u tr·ªØ d·ªØ li·ªáu c·ªßa c√°c vua t√¨m ƒë∆∞·ª£c
    seed_data = []
    
    # M·ªü file XML ƒë·ªÉ ƒë·ªçc
    with open(dump_path, 'r', encoding='utf-8') as f:
        # S·ª≠ d·ª•ng iterparse ƒë·ªÉ ƒë·ªçc XML theo t·ª´ng ph·∫ßn, ti·∫øt ki·ªám b·ªô nh·ªõ
        context = ET.iterparse(f, events=('end',))
        
        for event, elem in context:
            # Namespace c·ªßa MediaWiki XML
            namespace = '{http://www.mediawiki.org/xml/export-0.10/}'
            
            # Khi m·ªôt th·∫ª <page> k·∫øt th√∫c, ch√∫ng ta x·ª≠ l√Ω n√≥
            if elem.tag == f'{namespace}page':
                id_elem = elem.find(f'{namespace}id')
                page_id = int(id_elem.text)
                
                # Ki·ªÉm tra xem page_id c√≥ n·∫±m trong t·∫≠p h·∫°t gi·ªëng kh√¥ng
                if page_id in target_ids:
                    title_elem = elem.find(f'{namespace}title')
                    text_elem = elem.find(f'{namespace}revision/{namespace}text')
                    
                    if title_elem is None or text_elem is None or text_elem.text is None:
                        elem.clear()
                        continue
                        
                    title = title_elem.text
                    print(f"  -> T√¨m th·∫•y: {title} (ID: {page_id})")
                    
                    # Ph√¢n t√≠ch wikitext
                    wikicode = mwparserfromhell.parse(text_elem.text)
                    
                    # Tr√≠ch xu·∫•t d·ªØ li·ªáu
                    infobox = extract_infobox_data(wikicode)
                    links = [link.title.strip() for link in wikicode.filter_wikilinks()]
                    
                    # Th√™m d·ªØ li·ªáu v√†o danh s√°ch k·∫øt qu·∫£
                    seed_data.append({
                        'page_id': page_id,
                        'title': title,
                        'infobox': infobox,
                        'links': links
                    })
                    
                    # X√≥a ID ƒë√£ t√¨m th·∫•y ƒë·ªÉ tƒÉng t·ªëc v√† d·ª´ng s·ªõm
                    target_ids.remove(page_id)
                    
                # Gi·∫£i ph√≥ng b·ªô nh·ªõ
                elem.clear()
                
                # N·∫øu ƒë√£ t√¨m th·∫•y t·∫•t c·∫£, d·ª´ng vi·ªác ƒë·ªçc file
                if not target_ids:
                    print("\nüéâ ƒê√£ t√¨m th·∫•y t·∫•t c·∫£ c√°c v·ªã vua trong t·∫≠p h·∫°t gi·ªëng! D·ª´ng x·ª≠ l√Ω.")
                    break
    
    print(f"\n‚úÖ Ho√†n th√†nh! T√¨m th·∫•y {len(seed_data)}/{len(NGUYEN_KINGS_PAGES)} nh√¢n v·∫≠t.")

    # L∆∞u danh s√°ch ban ƒë·∫ßu v√†o m·ªôt file JSON m·ªõi
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(seed_data, f, indent=4, ensure_ascii=False)
    
    print(f"Danh s√°ch h·∫°t gi·ªëng ban ƒë·∫ßu ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: '{output_path}'")

if __name__ == '__main__':
    XML_DUMP_PATH = '../../data/raw/viwiki-latest-pages-articles.xml'
    OUTPUT_SEED_PATH = '../../data/processed/seed_data_nguyen_kings.json'
    
    if not os.path.exists(XML_DUMP_PATH):
        print(f"‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y file XML t·∫°i '{XML_DUMP_PATH}'.")
        print("H√£y ch·∫Øc ch·∫Øn r·∫±ng b·∫°n ƒë√£ gi·∫£i n√©n file dump v√†o ƒë√∫ng th∆∞ m·ª•c.")
    else:
        create_seed_list_from_xml(XML_DUMP_PATH, NGUYEN_KINGS_PAGES, OUTPUT_SEED_PATH)