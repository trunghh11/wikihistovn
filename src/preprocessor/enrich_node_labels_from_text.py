import json
import os
import re
from typing import Dict, List

from config_paths import DATA_PROCESSED


# === Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ===

NODES_IN = os.path.join(DATA_PROCESSED, "network_nodes_full.json")
TEXTS_IN = os.path.join(DATA_PROCESSED, "network_nodes_texts.jsonl")
NODES_OUT = os.path.join(DATA_PROCESSED, "network_nodes_enriched.json")


def load_nodes(path: str) -> List[Dict]:
    print(f"Äang Ä‘á»c nodes tá»«: {path}")
    with open(path, "r", encoding="utf-8") as f:
        nodes = json.load(f)
    if not isinstance(nodes, list):
        raise ValueError("File nodes pháº£i chá»©a má»™t list cÃ¡c node.")
    print(f"  > Sá»‘ node: {len(nodes)}")
    return nodes


def load_texts(path: str) -> Dict[int, Dict]:
    """
    Äá»c corpus Wikipedia (JSONL) vÃ  tráº£ vá» dict:
      {page_id: {"intro_text": str, "plain_text": str}}
    """
    print(f"Äang Ä‘á»c corpus vÄƒn báº£n tá»«: {path}")
    texts: Dict[int, Dict] = {}
    count = 0
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
            except json.JSONDecodeError:
                continue
            pid = rec.get("page_id")
            if isinstance(pid, int):
                texts[pid] = {
                    "intro_text": rec.get("intro_text", "") or "",
                    "plain_text": rec.get("plain_text", "") or "",
                }
                count += 1
    print(f"  > Äá»c Ä‘Æ°á»£c text cho {count} page_id")
    return texts


def classify_label_from_text(intro_text: str, current_label: str) -> str:
    """
    PhÃ¢n loáº¡i Ä‘Æ¡n giáº£n dá»±a trÃªn intro_text.
    Chá»‰ "sá»­a" láº¡i náº¿u current_label Ä‘ang lÃ  nhÃ£n ráº¥t chung nhÆ° 'Thá»±c thá»ƒ'.
    """
    if current_label and current_label not in ("Thá»±c thá»ƒ", "Entity", "Other"):
        # ÄÃ£ cÃ³ nhÃ£n khÃ¡ rÃµ (Vua NhÃ  Nguyá»…n, NhÃ¢n váº­t Lá»‹ch sá»­, Sá»± kiá»‡n...)
        return current_label

    if not intro_text:
        return current_label or "Thá»±c thá»ƒ"

    text = intro_text.lower()

    # Heuristic cho Äá»ŠA DANH
    place_keywords = [
        "lÃ  má»™t tá»‰nh",
        "lÃ  má»™t thÃ nh phá»‘",
        "lÃ  má»™t huyá»‡n",
        "lÃ  má»™t quáº­n",
        "lÃ  má»™t xÃ£",
        "lÃ  má»™t phÆ°á»ng",
        "lÃ  má»™t thá»‹ tráº¥n",
        "lÃ  má»™t quá»‘c gia",
        "lÃ  má»™t tiá»ƒu bang",
        "lÃ  má»™t vÃ¹ng",
        "lÃ  má»™t Ä‘áº£o",
        "lÃ  má»™t bÃ¡n Ä‘áº£o",
        "lÃ  má»™t ngá»n nÃºi",
        "lÃ  má»™t con sÃ´ng",
        "lÃ  má»™t há»“",
        "lÃ  má»™t vá»‹nh",
    ]
    if any(k in text for k in place_keywords):
        return "Äá»‹a danh"

    # Heuristic cho Sá»° KIá»†N
    event_keywords = [
        "lÃ  má»™t tráº­n",
        "lÃ  tráº­n",
        "lÃ  má»™t cuá»™c chiáº¿n",
        "lÃ  má»™t cuá»™c chiáº¿n tranh",
        "lÃ  má»™t cuá»™c khá»Ÿi nghÄ©a",
        "lÃ  má»™t khá»Ÿi nghÄ©a",
        "lÃ  má»™t chiáº¿n dá»‹ch",
        "lÃ  má»™t phong trÃ o",
        "lÃ  má»™t sá»± kiá»‡n",
        "lÃ  má»™t vá»¥",
    ]
    if any(k in text for k in event_keywords):
        return "Sá»± kiá»‡n"

    # Heuristic cho Tá»” CHá»¨C / THIáº¾T CHáº¾
    org_keywords = [
        "lÃ  má»™t Ä‘áº£ng chÃ­nh trá»‹",
        "lÃ  má»™t tá»• chá»©c",
        "lÃ  má»™t cÆ¡ quan",
        "lÃ  má»™t trÆ°á»ng Ä‘áº¡i há»c",
        "lÃ  má»™t trÆ°á»ng cao Ä‘áº³ng",
        "lÃ  má»™t cÃ¢u láº¡c bá»™",
        "lÃ  má»™t doanh nghiá»‡p",
        "lÃ  má»™t cÃ´ng ty",
        "lÃ  má»™t táº­p Ä‘oÃ n",
        "lÃ  má»™t tá» bÃ¡o",
        "lÃ  má»™t nháº­t bÃ¡o",
        "lÃ  má»™t táº¡p chÃ­",
    ]
    if any(k in text for k in org_keywords):
        return "Tá»• chá»©c"

    # Heuristic cho NHÃ‚N Váº¬T Lá»ŠCH Sá»¬
    person_patterns = [
        r"lÃ  má»™t .*nhÃ  [a-zÃ Ã¡áº¡Ã£áº£Äƒáº¯áº±áº·áºµáº³Ã¢áº¥áº§áº­áº«áº©Ãªáº¿á»á»‡á»…á»ƒÃ´á»‘á»“á»™á»—á»•Æ¡á»›á»á»£á»¡á»ŸÆ°á»©á»«á»±á»¯á»­]+",  # nhÃ  vÄƒn, nhÃ  thÆ¡...
        r"lÃ  má»™t .*hoÃ ng Ä‘áº¿",
        r"lÃ  má»™t .*vua",
        r"lÃ  má»™t .*hoÃ ng háº­u",
        r"lÃ  má»™t .*tÆ°á»›ng",
        r"lÃ  má»™t .*quan láº¡i",
        r"sinh nÄƒm \d{3,4}",
    ]
    if any(re.search(pat, text) for pat in person_patterns):
        return "NhÃ¢n váº­t Lá»‹ch sá»­"

    # Náº¿u khÃ´ng nháº­n dáº¡ng Ä‘Æ°á»£c thÃ¬ giá»¯ nguyÃªn / gÃ¡n Thá»±c thá»ƒ
    return current_label or "Thá»±c thá»ƒ"


def enrich_labels() -> None:
    nodes = load_nodes(NODES_IN)
    texts = load_texts(TEXTS_IN)

    updated = 0
    for node in nodes:
        pid = node.get("page_id")
        current_label = node.get("label", "") or ""
        intro = ""
        if isinstance(pid, int) and pid in texts:
            intro = texts[pid]["intro_text"]

        new_label = classify_label_from_text(intro, current_label)
        if new_label != current_label:
            node["label"] = new_label
            updated += 1

    os.makedirs(os.path.dirname(NODES_OUT), exist_ok=True)
    with open(NODES_OUT, "w", encoding="utf-8") as f:
        json.dump(nodes, f, ensure_ascii=False, indent=4)

    print(f"âœ… ÄÃ£ cáº­p nháº­t nhÃ£n cho {updated} node.")
    print(f"   File output: {NODES_OUT}")


if __name__ == "__main__":
    print("--- ğŸš€ LÃ m giÃ u nhÃ£n node tá»« vÄƒn báº£n Wikipedia (intro_text) ---")
    try:
        enrich_labels()
        print("\n--- HoÃ n táº¥t enrich_node_labels_from_text ---")
    except FileNotFoundError as e:
        print(f"âŒ Thiáº¿u file Ä‘áº§u vÃ o: {e}")
        print("   HÃ£y Ä‘áº£m báº£o Ä‘Ã£ cÃ³ network_nodes_full.json vÃ  network_nodes_texts.jsonl.")
    except Exception as e:
        print(f"âŒ ÄÃ£ xáº£y ra lá»—i: {e}")


