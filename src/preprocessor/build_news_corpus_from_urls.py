import json
import os
import time
from typing import List, Dict

import requests
from bs4 import BeautifulSoup

from config_paths import DATA_RAW, DATA_PROCESSED


# === C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N ===

# File input: m·ªói d√≤ng l√† m·ªôt URL b√†i b√°o
NEWS_URLS_TXT = os.path.join(DATA_RAW, "news_urls.txt")

# File output: corpus b√°o ch√≠ d·∫°ng JSON Lines
NEWS_CORPUS_OUT = os.path.join(DATA_PROCESSED, "news_corpus.jsonl")


HEADERS = {
    "User-Agent": "VietnameseHistoryNewsCollector/1.0 (Project for university; contact: 22024527@vnu.edu.vn)"
}


def load_urls(path: str) -> List[str]:
    """ƒê·ªçc danh s√°ch URL t·ª´ file txt (m·ªói d√≤ng 1 URL, b·ªè d√≤ng tr·ªëng / comment #)."""
    urls: List[str] = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            urls.append(line)
    print(f"ƒê√£ ƒë·ªçc {len(urls)} URL b√†i b√°o t·ª´ {path}")
    return urls


def extract_meta(soup: BeautifulSoup, *names_or_props: str) -> str:
    """Ti·ªán √≠ch l·∫•y meta theo name/property trong HTML."""
    for key in names_or_props:
        tag = soup.find("meta", attrs={"name": key}) or soup.find(
            "meta", attrs={"property": key}
        )
        if tag and tag.get("content"):
            return tag["content"].strip()
    return ""


def extract_article_text(soup: BeautifulSoup) -> str:
    """
    C·ªë g·∫Øng b√≥c ph·∫ßn th√¢n b√†i b√°o.
    - ∆Øu ti√™n th·∫ª <article>, sau ƒë√≥ c√°c <div> c√≥ class g·ª£i √Ω ('content', 'article', 'body').
    - Fallback: gh√©p t·∫•t c·∫£ c√°c <p> trong body.
    """
    # 1. <article>
    article = soup.find("article")
    if article:
        paragraphs = [p.get_text(" ", strip=True) for p in article.find_all("p")]
        text = "\n".join(p for p in paragraphs if p)
        if text:
            return text

    # 2. div v·ªõi class 'content', 'article', 'body'
    candidate_classes = ["content", "article", "body", "main-content", "news-content"]
    for div in soup.find_all("div"):
        class_list = " ".join(div.get("class", [])).lower()
        if any(c in class_list for c in candidate_classes):
            paragraphs = [p.get_text(" ", strip=True) for p in div.find_all("p")]
            text = "\n".join(p for p in paragraphs if p)
            if text:
                return text

    # 3. Fallback: t·∫•t c·∫£ <p> trong body
    body = soup.find("body") or soup
    paragraphs = [p.get_text(" ", strip=True) for p in body.find_all("p")]
    text = "\n".join(p for p in paragraphs if p)
    return text


def fetch_article(url: str) -> Dict:
    """T·∫£i v√† b√≥c t√°ch 1 b√†i b√°o. Tr·∫£ v·ªÅ dict v·ªõi title, published_date, source, text."""
    print(f"  > ƒêang t·∫£i: {url}")
    try:
        resp = requests.get(url, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"    ! L·ªói khi t·∫£i URL: {e}")
        return {}

    soup = BeautifulSoup(resp.content, "html.parser")

    # Ti√™u ƒë·ªÅ
    title = extract_meta(soup, "og:title", "twitter:title") or (
        soup.title.get_text(strip=True) if soup.title else ""
    )

    # Ng√†y ƒëƒÉng (best-effort, t√πy t·ª´ng b√°o)
    published = extract_meta(
        soup,
        "article:published_time",
        "pubdate",
        "publishdate",
        "date",
        "dcterms.date",
    )

    # Ngu·ªìn (domain)
    try:
        from urllib.parse import urlparse

        source = urlparse(url).netloc
    except Exception:
        source = ""

    text = extract_article_text(soup)

    if not text:
        print("    ‚ö†Ô∏è Kh√¥ng b√≥c ƒë∆∞·ª£c n·ªôi dung ch√≠nh, b·ªè qua.")
        return {}

    return {
        "url": url,
        "source": source,
        "title": title,
        "published": published,
        "text": text,
    }


def build_news_corpus(urls_path: str, out_path: str) -> None:
    """
    T·ª´ danh s√°ch URL b√°o ch√≠, x√¢y m·ªôt corpus b√°o ch√≠ d·∫°ng JSON Lines ƒë·ªÉ l√†m gi√†u d·ªØ li·ªáu.

    M·ªói d√≤ng trong out_path:
      {
        "id": int,
        "url": "...",
        "source": "vnexpress.net",
        "title": "...",
        "published": "2024-01-01T10:00:00+07:00" (n·∫øu b√≥c ƒë∆∞·ª£c),
        "text": "N·ªôi dung ch√≠nh c·ªßa b√†i b√°o..."
      }
    """
    urls = load_urls(urls_path)
    if not urls:
        print("‚ùå Kh√¥ng c√≥ URL n√†o trong file news_urls.txt.")
        return

    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    written = 0
    with open(out_path, "w", encoding="utf-8") as fout:
        for idx, url in enumerate(urls, start=1):
            article = fetch_article(url)
            # L·ªãch s·ª± v·ªõi server
            time.sleep(1)
            if not article:
                continue

            record = {"id": idx, **article}
            fout.write(json.dumps(record, ensure_ascii=False) + "\n")
            written += 1

    print(f"\n‚úÖ ƒê√£ ghi {written}/{len(urls)} b√†i b√°o v√†o: {out_path}")


if __name__ == "__main__":
    print("--- üöÄ X√¢y d·ª±ng corpus b√°o ch√≠ t·ª´ danh s√°ch URL ---")
    if not os.path.exists(NEWS_URLS_TXT):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file URL t·∫°i: {NEWS_URLS_TXT}")
        print("   H√£y t·∫°o file 'data/raw/news_urls.txt', m·ªói d√≤ng l√† m·ªôt URL b√†i b√°o c·∫ßn thu th·∫≠p.")
    else:
        try:
            build_news_corpus(NEWS_URLS_TXT, NEWS_CORPUS_OUT)
            print("\n--- Ho√†n t·∫•t build_news_corpus_from_urls ---")
        except Exception as e:
            print(f"‚ùå ƒê√£ x·∫£y ra l·ªói: {e}")


