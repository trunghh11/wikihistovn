import requests
import json
import time
from tqdm import tqdm
import os

# --- C·∫§U H√åNH ---
INPUT_NODES_FILE = "data/processed/nodes_metadata.json"  # File ƒë·∫ßu v√†o t·ª´ Step 1
OUTPUT_NODES_FILE = "data/processed/nodes_metadata_enriched.json" # File ƒë·∫ßu ra c√≥ th√™m summary

HEADERS = {
    "User-Agent": "VietnameseHistoryNetwork/1.0 (Summary fetcher; contact: student@vnu.edu.vn)"
}

def fetch_wiki_summary(title):
    """
    L·∫•y ƒëo·∫°n m·ªü ƒë·∫ßu (summary/extract) c·ªßa b√†i vi·∫øt Wikipedia ti·∫øng Vi·ªát theo ti√™u ƒë·ªÅ.
    """
    url = "https://vi.wikipedia.org/w/api.php"
    params = {
        "action": "query",
        "format": "json",
        "titles": title,
        "prop": "extracts",
        "exintro": True,       # Ch·ªâ l·∫•y ph·∫ßn m·ªü ƒë·∫ßu (introduction)
        "explaintext": True,   # L·∫•y vƒÉn b·∫£n thu·∫ßn (plaintext)
        "redirects": True      # Theo d√µi c√°c trang ƒë·ªïi h∆∞·ªõng
    }
    
    try:
        response = requests.get(url, params=params, headers=HEADERS, timeout=10)
        
        # N·∫øu g·∫∑p l·ªói server (5xx) ho·∫∑c Client (4xx), tr·∫£ v·ªÅ chu·ªói r·ªóng ƒë·ªÉ kh√¥ng ch·∫øt ch∆∞∆°ng tr√¨nh
        if response.status_code != 200:
            return ""

        data = response.json()
        
        # X·ª≠ l√Ω k·∫øt qu·∫£ tr·∫£ v·ªÅ
        if 'query' not in data or 'pages' not in data['query']:
            return ""

        page = next(iter(data['query']['pages'].values()))
        
        if 'missing' in page:
            return ""
        
        # L·∫•y extract, n·∫øu kh√¥ng c√≥ th√¨ tr·∫£ v·ªÅ r·ªóng
        summary = page.get('extract', "")
        return summary

    except Exception as e:
        print(f"\n[L·ªói] {title}: {e}")
        return ""

def main():
    # 1. Ki·ªÉm tra file ƒë·∫ßu v√†o
    if not os.path.exists(INPUT_NODES_FILE):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {INPUT_NODES_FILE}. H√£y ch·∫°y Step 1 tr∆∞·ªõc.")
        return

    print(f"--- ƒêANG ƒê·ªåC D·ªÆ LI·ªÜU T·ª™ {INPUT_NODES_FILE} ---")
    with open(INPUT_NODES_FILE, 'r', encoding='utf-8') as f:
        nodes = json.load(f)

    print(f"üîπ T·ªïng s·ªë node c·∫ßn x·ª≠ l√Ω: {len(nodes)}")
    
    # 2. Duy·ªát qua t·ª´ng node v√† l·∫•y summary
    # S·ª≠ d·ª•ng tqdm ƒë·ªÉ hi·ªán thanh ti·∫øn tr√¨nh
    updated_nodes = []
    
    for node in tqdm(nodes, desc="Fetching Summaries"):
        title = node.get('title', '')
        
        # N·∫øu ch∆∞a c√≥ summary ho·∫∑c summary r·ªóng th√¨ m·ªõi fetch
        if 'summary' not in node or not node['summary']:
            summary = fetch_wiki_summary(title)
            # L√†m s·∫°ch summary (x√≥a xu·ªëng d√≤ng th·ª´a)
            node['summary'] = summary.replace('\n', ' ').strip()
        
        updated_nodes.append(node)
        
        # Delay nh·∫π 0.1s ƒë·ªÉ t√¥n tr·ªçng server Wikipedia (tr√°nh l·ªói 429 Too Many Requests)
        time.sleep(0.1)

    # 3. L∆∞u k·∫øt qu·∫£
    print(f"\n--- ƒêANG L∆ØU K·∫æT QU·∫¢ RA {OUTPUT_NODES_FILE} ---")
    with open(OUTPUT_NODES_FILE, 'w', encoding='utf-8') as f:
        json.dump(updated_nodes, f, ensure_ascii=False, indent=4)
    
    print(f"‚úÖ Ho√†n t·∫•t! ƒê√£ th√™m summary cho {len(updated_nodes)} nodes.")

if __name__ == "__main__":
    main()