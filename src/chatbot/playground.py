import sys
import json
import gradio as gr
from neo4j import GraphDatabase
from mlx_lm import load, generate

# ============================
# CONFIG
# ============================

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASS = "12345678"
MODEL_ID   = "Qwen/Qwen3-0.6B-MLX-bf16"

# ============================
# LOAD MODEL
# ============================

print("\n>>> ‚è≥ ƒêang kh·ªüi t·∫°o model MLX Playground...")
try:
    model, tokenizer = load(MODEL_ID)
    print("    - MLX Model ƒë√£ load th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªói load MLX model: {e}")
    sys.exit(1)

# ============================
# CONNECT NEO4J
# ============================

try:
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))
    driver.verify_connectivity()
    print("    - Neo4j ƒë√£ k·∫øt n·ªëi!")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi Neo4j: {e}")
    sys.exit(1)


# ============================
# NEO4J QUERIES
# ============================

def query_summary(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    RETURN node.title as name, node.summary as summary
    LIMIT 1
    """
    with driver.session() as s:
        r = s.run(cypher, kw=keyword).single()
        if r and r["summary"]:
            return f"T√ìM T·∫ÆT V·ªÄ {r['name']}:\n{r['summary']}"
    return None


def query_relations(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    WITH node LIMIT 1
    MATCH (node)-[r]-(n1)
    WHERE NOT type(r) IN ['LI√äN_K·∫æT_T·ªöI']
    RETURN node.title AS center, type(r) AS rel_type, n1.title AS neighbor
    LIMIT 30
    """
    rows = []
    with driver.session() as s:
        for r in s.run(cypher, kw=keyword):
            rows.append(f"- {r['center']} --[{r['rel_type']}]--> {r['neighbor']}")
    return "\n".join(rows) if rows else None


# ============================
# MLX GENERATE
# ============================

def run_mlx(prompt, max_tokens=128):
    output = generate(
        model,
        tokenizer,
        prompt=prompt,
        max_tokens=max_tokens,
        verbose=False
    )
    return output.strip()


# ============================
# INTENT DETECTOR
# ============================

def detect_intent_and_keyword(question):
    prompt = f"""B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch c√¢u h·ªèi l·ªãch s·ª≠. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v√† tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON.

ƒê·ªãnh nghƒ©a Intent (√ù ƒë·ªãnh):
1. "SUMMARY": Khi ng∆∞·ªùi d√πng h·ªèi th√¥ng tin chung, ti·ªÉu s·ª≠, ƒë·ªãnh nghƒ©a.
   - T·ª´ kh√≥a nh·∫≠n bi·∫øt: "l√† ai", "ti·ªÉu s·ª≠", "gi·ªõi thi·ªáu", "cu·ªôc ƒë·ªùi", "th√¥ng tin", "s·ª± nghi·ªáp", "sinh nƒÉm n√†o", "m·∫•t nƒÉm n√†o".
2. "RELATION": Khi ng∆∞·ªùi d√πng h·ªèi v·ªÅ m·ªëi quan h·ªá gi·ªØa c√°c nh√¢n v·∫≠t ho·∫∑c ch·ª©c v·ª•, vai tr√≤.
   - T·ª´ kh√≥a nh·∫≠n bi·∫øt: "cha", "m·∫π", "con", "v·ª£", "ch·ªìng", "anh", "em", "k·∫ø nhi·ªám", "ti·ªÅn nhi·ªám", "th·∫ßy", "tr√≤", "quan h·ªá", "l√† g√¨ c·ªßa".

V√≠ d·ª• m·∫´u (H√£y h·ªçc theo c√°ch ph√¢n t√≠ch n√†y):
- C√¢u h·ªèi: "Vua Gia Long l√† ai?"
  -> {{"intent": "SUMMARY", "keyword": "Gia Long"}}

- C√¢u h·ªèi: "Cha c·ªßa vua Minh M·∫°ng l√† ai?"
  -> {{"intent": "RELATION", "keyword": "Minh M·∫°ng"}} (L∆∞u √Ω: L·∫•y t√™n nh√¢n v·∫≠t ƒë√£ bi·∫øt, kh√¥ng l·∫•y t·ª´ "Cha")

- C√¢u h·ªèi: "Ai l√† v·ª£ c·ªßa vua B·∫£o ƒê·∫°i?"
  -> {{"intent": "RELATION", "keyword": "B·∫£o ƒê·∫°i"}}

- C√¢u h·ªèi: "H√£y t√≥m t·∫Øt ti·ªÉu s·ª≠ Tr·∫ßn H∆∞ng ƒê·∫°o"
  -> {{"intent": "SUMMARY", "keyword": "Tr·∫ßn H∆∞ng ƒê·∫°o"}}

- C√¢u h·ªèi: "Nguy·ªÖn Hu·ªá v√† Nguy·ªÖn Nh·∫°c c√≥ quan h·ªá g√¨?"
  -> {{"intent": "RELATION", "keyword": "Nguy·ªÖn Hu·ªá"}}

Y√™u c·∫ßu output:
- Ch·ªâ tr·∫£ v·ªÅ 1 JSON duy nh·∫•t.
- Kh√¥ng gi·∫£i th√≠ch th√™m.
- Keyword ch·ªâ ch·ª©a t√™n ri√™ng, lo·∫°i b·ªè c√°c t·ª´ nh∆∞ "vua", "√¥ng", "b√†" n·∫øu kh√¥ng c·∫ßn thi·∫øt.

C√¢u h·ªèi c·∫ßn ph√¢n t√≠ch: "{question}"
JSON Output:"""

    if tokenizer.chat_template:
        messages = [{"role": "user", "content": prompt}]
        final_prompt = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
    else:
        final_prompt = prompt

    raw = run_mlx(final_prompt, max_tokens=64)

    try:
        json_part = raw[raw.find("{") : raw.rfind("}") + 1]
        return json.loads(json_part)
    except:
        return {"intent": "RELATION", "keyword": question}


# ============================
# RAG PIPELINE
# ============================

def generate_rag_response(question):
    # 1. Router
    analysis = detect_intent_and_keyword(question)
    intent = analysis.get("intent", "RELATION")
    keyword = analysis.get("keyword", question)

    print(f"\n[DEBUG] Intent: {intent} | Keyword: {keyword}")

    # 2. Retriever
    if intent == "SUMMARY":
        context = query_summary(keyword)
        if not context:
            context = query_relations(keyword)
            intent = "RELATION (Fallback)"
    else:
        context = query_relations(keyword)

    if not context:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu."

    # 3. Generator
    db_context = f"""TH√îNG TIN T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU ({intent}):
---------------------
{context}
---------------------"""

    user_prompt = f"{db_context}\n\nD·ª±a v√†o th√¥ng tin tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn:"

    if hasattr(tokenizer, "apply_chat_template") and tokenizer.chat_template:
        messages = [
            {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω l·ªãch s·ª≠ Vi·ªát Nam trung th·ª±c. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p."},
            {"role": "user", "content": user_prompt},
        ]
        final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        final_prompt = user_prompt

    # TƒÉng max_tokens cho c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
    answer = run_mlx(final_prompt, max_tokens=512)
    return answer.strip(), analysis, db_context


# ============================
# GRADIO PLAYGROUND UI
# ============================

def gradio_process(question):
    answer, router, ctx = generate_rag_response(question)
    return (
        answer,
        json.dumps(router, indent=2, ensure_ascii=False),
        ctx
    )

css = """
.gr-textinput {font-size: 18px !important;}
"""

with gr.Blocks(css=css, title="MLX RAG Playground") as demo:

    gr.Markdown("# ü§ñ **Vietnam History MLX Playground**\nRAG + Qwen3-0.6B + Neo4j + MLX")

    with gr.Row():
        question = gr.Textbox(label="Nh·∫≠p c√¢u h·ªèi", placeholder="V√≠ d·ª•: Cha c·ªßa Minh M·∫°ng l√† ai?", lines=2)

    run_btn = gr.Button("üöÄ Generate")

    with gr.Row():
        ans_box = gr.Textbox(label="Tr·∫£ l·ªùi t·ª´ Bot", lines=7)
    with gr.Row():
        router_box = gr.Textbox(label="Router (Intent + Keyword)", lines=6)
    with gr.Row():
        ctx_box = gr.Textbox(label="Context l·∫•y t·ª´ Neo4j", lines=12)

    run_btn.click(
        fn=gradio_process,
        inputs=[question],
        outputs=[ans_box, router_box, ctx_box]
    )

demo.launch(server_name="0.0.0.0", server_port=7860)
