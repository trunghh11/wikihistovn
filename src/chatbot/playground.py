import os
# --- 1. FIX Lá»–I CRASH TRÃŠN MAC (TQDM) ---
os.environ["HF_HUB_DISABLE_PROGRESS_BARS"] = "1"

import sys
import json
import gradio as gr
from neo4j import GraphDatabase
from mlx_lm import load, generate

# ============================
# CONFIG
# ============================

NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASS = "12345678"
MODEL_ID   = "Qwen/Qwen3-0.6B-MLX-bf16"

# ============================
# LOAD MODEL
# ============================

print("\n>>> â³ Äang khá»Ÿi táº¡o model MLX Playground...")
try:
    model, tokenizer = load(MODEL_ID)
    print("    - MLX Model Ä‘Ã£ load thÃ nh cÃ´ng!")
except Exception as e:
    print(f"âŒ Lá»—i load MLX model: {e}")
    sys.exit(1)

# ============================
# CONNECT NEO4J
# ============================

try:
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))
    driver.verify_connectivity()
    print("    - Neo4j Ä‘Ã£ káº¿t ná»‘i!")
except Exception as e:
    print(f"âŒ Lá»—i káº¿t ná»‘i Neo4j: {e}")
    sys.exit(1)


# ============================
# NEO4J QUERIES (SAFE MODE)
# ============================

def query_summary(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    RETURN node.title as name, node.summary as summary
    LIMIT 1
    """
    with driver.session() as s:
        # DÃ¹ng single() an toÃ n
        r = s.run(cypher, kw=keyword).single()
        if r and r["summary"]:
            return f"TÃ“M Táº®T Vá»€ {r['name']}:\n{r['summary']}"
    return None


def query_1hop(keyword):
    """Truy váº¥n quan há»‡ trá»±c tiáº¿p"""
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    WITH node LIMIT 1
    MATCH (node)-[r]-(n1)
    WHERE NOT type(r) IN ['LIÃŠN_Káº¾T_Tá»šI']
    RETURN node.title AS center, type(r) AS rel_type, n1.title AS neighbor
    LIMIT 30
    """
    with driver.session() as s:
        # DÃ¹ng list() Ä‘á»ƒ láº¥y háº¿t dá»¯ liá»‡u trÆ°á»›c khi Ä‘Ã³ng session
        results = list(s.run(cypher, kw=keyword))
        
    rows = [f"- {r['center']} --[{r['rel_type']}]--> {r['neighbor']}" for r in results]
    return "\n".join(rows) if rows else None


def query_2hop(keyword):
    """Truy váº¥n quan há»‡ báº¯c cáº§u (Multi-hop)"""
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    WITH node LIMIT 1
    MATCH path = (node)-[*1..2]-(m)
    WHERE NONE(r IN relationships(path) WHERE type(r) IN ['LIÃŠN_Káº¾T_Tá»šI'])
    AND m.title <> node.title
    RETURN path
    LIMIT 50
    """
    paths_text = []
    with driver.session() as s:
        results = list(s.run(cypher, kw=keyword))
        
        for record in results:
            path = record["path"]
            nodes = path.nodes
            rels = path.relationships
            chain = []
            for i in range(len(rels)):
                start = nodes[i].get("title", "Unknown")
                end = nodes[i+1].get("title", "Unknown")
                rel_type = rels[i].type
                chain.append(f"{start} --[{rel_type}]--> {end}")
            paths_text.append(" ; ".join(chain))
            
    return "\n".join(list(set(paths_text))) if paths_text else None


# ============================
# MLX GENERATE (COMPATIBILITY)
# ============================

def run_mlx(prompt, max_tokens=128):
    return generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens, verbose=False).strip()


# ============================
# INTENT DETECTOR (SMART ROUTER)
# ============================

def detect_intent_and_keyword(question):
    # Prompt nÃ¢ng cao Ä‘á»ƒ nháº­n diá»‡n cáº£ sá»‘ bÆ°á»›c nháº£y (Hops)
    prompt = f"""PhÃ¢n tÃ­ch cÃ¢u há»i sau vÃ  tráº£ vá» Ä‘á»‹nh dáº¡ng JSON duy nháº¥t.
CÃ¢u há»i: "{question}"

YÃªu cáº§u:
1. "intent": "SUMMARY" (Náº¿u há»i nÄƒm sinh, nÄƒm máº¥t, quÃª quÃ¡n) Hoáº·c "RELATION" (náº¿u há»i quan há»‡) .
2. "keyword": TÃªn nhÃ¢n váº­t chÃ­nh trong cÃ¢u há»i.
3. "hops": 1 hoáº·c 2.

VÃ­ dá»¥: "NÄƒm sinh cá»§a Minh Máº¡ng lÃ  bao nhiÃªu?" -> {{"intent": "SUMMARY", "keyword": "Minh Máº¡ng", "hops": 2}}

JSON Output:"""

    if hasattr(tokenizer, "apply_chat_template") and tokenizer.chat_template:
        messages = [{"role": "user", "content": prompt}]
        final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        final_prompt = prompt

    # DÃ¹ng temp=0 Ä‘á»ƒ output JSON á»•n Ä‘á»‹nh
    raw = run_mlx(final_prompt, max_tokens=128)

    try:
        json_part = raw[raw.find("{") : raw.rfind("}") + 1]
        return json.loads(json_part)
    except:
        # Fallback máº·c Ä‘á»‹nh
        return {"intent": "RELATION", "keyword": question, "hops": 1}


# ============================
# RAG PIPELINE
# ============================

def generate_rag_response(question):
    # 1. Router
    analysis = detect_intent_and_keyword(question)
    print(analysis)
    intent = analysis.get("intent", "")
    keyword = analysis.get("keyword", question)
    hops = analysis.get("hops", 1)

    print(f"\n[DEBUG] Intent: {intent} | Keyword: {keyword} | Hops: {hops}")

    # 2. Retriever
    context = None
    if intent == "SUMMARY":
        context = query_summary(keyword)
        if not context:
            context = query_1hop(keyword)
            intent = "RELATION (Fallback)"
    else:
        # Smart Hop Selection
        if hops >= 2:
            context = query_2hop(keyword)
        else:
            context = query_1hop(keyword)

    if not context:
        return "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.", analysis, "No Context Found"

    # 3. Generator
    instruction = ""
    if hops >= 2:
        instruction = "\nHÆ°á»›ng dáº«n: HÃ£y suy luáº­n báº¯c cáº§u (VÃ­ dá»¥: A lÃ  cha B, B lÃ  cha C => A lÃ  Ã´ng ná»™i C) Ä‘á»ƒ tráº£ lá»i."

    db_context_display = f"THÃ”NG TIN ({intent} - {hops} HOP):\n---------------------\n{context}\n---------------------"
    
    user_prompt = f"""Dá»® LIá»†U TRI THá»¨C:
----------------
{context}
----------------

CÃ¢u há»i: {question}
{instruction}
Tráº£ lá»i ngáº¯n gá»n:"""

    if hasattr(tokenizer, "apply_chat_template") and tokenizer.chat_template:
        messages = [
            {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ lá»‹ch sá»­ Viá»‡t Nam trung thá»±c. Chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p."},
            {"role": "user", "content": user_prompt},
        ]
        final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        final_prompt = user_prompt

    # Sinh cÃ¢u tráº£ lá»i (Temp=0.1 Ä‘á»ƒ Ã­t bá»‹a)
    answer = run_mlx(final_prompt, max_tokens=512)
    
    return answer, analysis, db_context_display


# ============================
# GRADIO PLAYGROUND UI
# ============================

def gradio_process(question):
    answer, router, ctx = generate_rag_response(question)
    return (
        answer,
        json.dumps(router, indent=2, ensure_ascii=False),
        ctx
    )

css = """
.gr-textinput {font-size: 16px !important;}
footer {visibility: hidden}
"""

with gr.Blocks(css=css, title="Sá»­ Viá»‡t Chatbot") as demo:

    gr.Markdown("# ðŸ‡»ðŸ‡³ **Playground Sá»­ Viá»‡t (MLX + Neo4j)**")
    gr.Markdown("Há»‡ thá»‘ng RAG há»— trá»£ suy luáº­n Multi-hop trÃªn chip Apple Silicon.")

    with gr.Row():
        with gr.Column(scale=4):
            question = gr.Textbox(label="CÃ¢u há»i", lines=2)
            run_btn = gr.Button("ðŸš€ Gá»­i cÃ¢u há»i", variant="primary")
        
        with gr.Column(scale=2):
            router_box = gr.JSON(label="ðŸ” PhÃ¢n tÃ­ch (Router)")

    with gr.Row():
        ans_box = gr.Textbox(label="ðŸ¤– Bot tráº£ lá»i", lines=5, show_copy_button=True)
    
    # with gr.Row():
    #     ctx_box = gr.Textbox(label="ðŸ“š Dá»¯ liá»‡u Graph trÃ­ch xuáº¥t (Context)", lines=10, max_lines=20)

    # Sá»± kiá»‡n
    run_btn.click(
        fn=gradio_process,
        inputs=[question],
        outputs=[ans_box, router_box]
    )
    # Cho phÃ©p áº¥n Enter Ä‘á»ƒ gá»­i
    question.submit(
        fn=gradio_process,
        inputs=[question],
        outputs=[ans_box, router_box]
    )

# Cháº¡y server
print(">>> ðŸš€ Gradio Ä‘ang cháº¡y táº¡i: http://localhost:7860")
demo.launch(server_name="0.0.0.0", server_port=7860)