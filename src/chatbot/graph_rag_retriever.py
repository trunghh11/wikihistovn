import json
import os
import re
import unicodedata
from collections import defaultdict, Counter, deque
from dataclasses import dataclass
from typing import Dict, List, Set, Tuple, Optional

from src.common.config_paths import DATA_PROCESSED


# === ƒê∆Ø·ªúNG D·∫™N D·ªÆ LI·ªÜU ===

NODES_IN = os.path.join(DATA_PROCESSED, "network_nodes_ranked.json")
RELS_BASE_IN = os.path.join(DATA_PROCESSED, "network_relationships_full.json")
RELS_TEXT_IN = os.path.join(DATA_PROCESSED, "network_relationships_text_based.json")
TEXTS_IN = os.path.join(DATA_PROCESSED, "network_nodes_texts.jsonl")
NEWS_IN = os.path.join(DATA_PROCESSED, "news_corpus.jsonl")


def strip_accents(text: str) -> str:
    """
    Chu·∫©n h√≥a ti·∫øng Vi·ªát: b·ªè d·∫•u, d√πng ƒë·ªÉ so kh·ªõp t·ª´ kh√≥a ƒë∆°n gi·∫£n.
    """
    text = unicodedata.normalize("NFD", text)
    text = "".join(ch for ch in text if unicodedata.category(ch) != "Mn")
    return unicodedata.normalize("NFC", text)


def normalize_for_match(text: str) -> str:
    """
    Normalize cho vi·ªác t√¨m ki·∫øm: lower + b·ªè d·∫•u + r√∫t g·ªçn kho·∫£ng tr·∫Øng.
    """
    text = text.lower()
    text = strip_accents(text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


@dataclass
class NodeInfo:
    title: str
    label: str
    pagerank: float
    combined_rank: int
    community_id: int
    community_size: int
    page_id: Optional[int] = None


class GraphRAGIndex:
    """
    Ch·ªâ m·ª•c ƒë∆°n gi·∫£n cho GraphRAG:
      - load node (k√®m pagerank, c·ªông ƒë·ªìng),
      - load c·∫°nh (g·ªëc + text),
      - load vƒÉn b·∫£n cho node + news,
      - cung c·∫•p API: t√¨m seed, m·ªü r·ªông subgraph, build context.
    """

    def __init__(
        self,
        nodes: Dict[str, NodeInfo],
        adj_undirected: Dict[str, Set[str]],
        node_texts: Dict[str, Dict[str, str]],
        news_docs: List[Dict],
    ):
        self.nodes = nodes
        self.adj = adj_undirected
        self.node_texts = node_texts
        self.news_docs = news_docs

        # Index ph·ª•c v·ª• search theo keyword
        self.title_index: Dict[str, Set[str]] = defaultdict(set)
        for title in nodes.keys():
            norm = normalize_for_match(title)
            self.title_index[norm].add(title)

    # ---------- LOAD T·ª™ FILE ----------

    @classmethod
    def from_files(cls) -> "GraphRAGIndex":
        # 1. Load nodes
        with open(NODES_IN, "r", encoding="utf-8") as f:
            raw_nodes = json.load(f)
        nodes: Dict[str, NodeInfo] = {}
        for n in raw_nodes:
            title = n.get("title")
            if not title:
                continue
            nodes[title] = NodeInfo(
                title=title,
                label=n.get("label", "Th·ª±c th·ªÉ"),
                pagerank=float(n.get("pagerank", 0.0)),
                combined_rank=int(n.get("combined_rank", 0)),
                community_id=int(n.get("community_id", 0)),
                community_size=int(n.get("community_size", 1)),
                page_id=n.get("page_id"),
            )

        # 2. Load relationships (g·ªëc + text) ‚Üí ƒë·ªì th·ªã v√¥ h∆∞·ªõng
        adj: Dict[str, Set[str]] = defaultdict(set)
        for path in (RELS_BASE_IN, RELS_TEXT_IN):
            if not os.path.exists(path):
                continue
            with open(path, "r", encoding="utf-8") as f:
                rels = json.load(f)
            for r in rels:
                s = r.get("source")
                t = r.get("target")
                if not s or not t:
                    continue
                if s not in nodes or t not in nodes:
                    continue
                if s == t:
                    continue
                adj[s].add(t)
                adj[t].add(s)

        # 3. Load vƒÉn b·∫£n cho node
        node_texts: Dict[str, Dict[str, str]] = {}
        if os.path.exists(TEXTS_IN):
            with open(TEXTS_IN, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                    except json.JSONDecodeError:
                        continue
                    title = rec.get("title")
                    if not title or title not in nodes:
                        continue
                    node_texts[title] = {
                        "intro_text": rec.get("intro_text", "") or "",
                        "plain_text": rec.get("plain_text", "") or "",
                    }

        # 4. Load corpus b√°o ch√≠ (tu·ª≥ ch·ªçn)
        news_docs: List[Dict] = []
        if os.path.exists(NEWS_IN):
            with open(NEWS_IN, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                        news_docs.append(rec)
                    except json.JSONDecodeError:
                        continue

        print(
            f"[GraphRAGIndex] Loaded {len(nodes)} nodes, {sum(len(v) for v in adj.values()) // 2} edges,"
            f" {len(node_texts)} nodes with text, {len(news_docs)} news docs."
        )

        return cls(nodes=nodes, adj_undirected=adj, node_texts=node_texts, news_docs=news_docs)

    # ---------- T√åM SEED NODES ----------

    def search_seeds(self, question: str, top_k: int = 5) -> List[str]:
        """
        T√¨m node seed t·ª´ c√¢u h·ªèi d·ª±a tr√™n:
          - match ti√™u ƒë·ªÅ (kh√¥ng d·∫•u + lower),
          - ƒë·∫øm t·∫ßn su·∫•t match, ∆∞u ti√™n PageRank cao.
        """
        q_norm = normalize_for_match(question)
        tokens = [tok for tok in q_norm.split(" ") if tok]

        scores: Counter = Counter()
        for title in self.nodes.keys():
            t_norm = normalize_for_match(title)
            # ƒë·∫øm s·ªë token c·ªßa c√¢u h·ªèi xu·∫•t hi·ªán trong title
            match_count = sum(1 for tok in tokens if tok in t_norm)
            if match_count > 0:
                scores[title] = match_count

        if not scores:
            return []

        # K·∫øt h·ª£p match_score + pagerank nh·ªè
        def sort_key(item):
            title, score = item
            pr = self.nodes[title].pagerank
            return (score, pr)

        ranked = sorted(scores.items(), key=sort_key, reverse=True)
        seeds = [title for title, _ in ranked[:top_k]]
        return seeds

    # ---------- M·ªû R·ªòNG SUBGRAPH ----------

    def expand_subgraph(
        self,
        seed_titles: List[str],
        max_depth: int = 2,
        max_nodes: int = 50,
    ) -> Set[str]:
        """
        BFS t·ª´ t·∫≠p seed tr√™n ƒë·ªì th·ªã v√¥ h∆∞·ªõng, l·∫•y h√†ng x√≥m t·ªõi ƒë·ªô s√¢u max_depth,
        gi·ªõi h·∫°n t·ªïng s·ªë node ƒë·ªÉ context kh√¥ng qu√° l·ªõn.
        """
        visited: Set[str] = set()
        queue = deque()

        for s in seed_titles:
            if s in self.nodes:
                visited.add(s)
                queue.append((s, 0))

        while queue and len(visited) < max_nodes:
            node, dist = queue.popleft()
            if dist >= max_depth:
                continue
            for nei in self.adj.get(node, []):
                if nei not in visited:
                    visited.add(nei)
                    queue.append((nei, dist + 1))
                    if len(visited) >= max_nodes:
                        break

        return visited

    # ---------- MULTI-HOP PATH REASONING TR√äN ƒê·ªí TH·ªä ----------

    def _shortest_path(
        self, src: str, dst: str, max_depth: int = 3
    ) -> Optional[List[str]]:
        """
        T√¨m m·ªôt ƒë∆∞·ªùng ƒëi ng·∫Øn nh·∫•t (<= max_depth c·∫°nh) gi·ªØa src v√† dst tr√™n ƒë·ªì th·ªã v√¥ h∆∞·ªõng.
        Tr·∫£ v·ªÅ danh s√°ch ti√™u ƒë·ªÅ node [src, ..., dst] ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y.
        """
        if src == dst:
            return [src]
        if src not in self.adj or dst not in self.adj:
            return None

        queue = deque([(src, 0)])
        prev: Dict[str, str] = {src: ""}  # node -> parent

        while queue:
            node, dist = queue.popleft()
            if dist >= max_depth:
                continue
            for nei in self.adj.get(node, []):
                if nei in prev:
                    continue
                prev[nei] = node
                if nei == dst:
                    # reconstruct path
                    path = [dst]
                    cur = dst
                    while prev[cur]:
                        cur = prev[cur]
                        path.append(cur)
                    path.reverse()
                    return path
                queue.append((nei, dist + 1))
        return None

    def extract_multi_hop_paths(
        self, seeds: List[str], max_depth: int = 3
    ) -> List[Dict]:
        """
        T√¨m c√°c ƒë∆∞·ªùng ƒëi ng·∫Øn (multi-hop) gi·ªØa c√°c seed nodes.
        Tr·∫£ v·ªÅ list c√°c path:
          [{"source": s, "target": t, "path": [s, ..., t]}]
        """
        seeds = [s for s in seeds if s in self.adj]
        paths: List[Dict] = []
        seen_pairs: Set[Tuple[str, str]] = set()

        for i in range(len(seeds)):
            for j in range(i + 1, len(seeds)):
                s, t = seeds[i], seeds[j]
                key = (s, t)
                if key in seen_pairs:
                    continue
                seen_pairs.add(key)
                path = self._shortest_path(s, t, max_depth=max_depth)
                if path and len(path) > 1:
                    paths.append({"source": s, "target": t, "path": path})
        return paths

    # ---------- X√ÇY CONTEXT CHO C√ÇU H·ªéI ----------

    def build_context_for_question(
        self,
        question: str,
        seed_top_k: int = 5,
        depth: int = 2,
        max_nodes: int = 40,
        max_news: int = 5,
        include_paths: bool = True,
    ) -> Dict:
        """
        Tr·∫£ v·ªÅ:
          - danh s√°ch node ƒë∆∞·ª£c ch·ªçn,
          - context text (Wiki + m·ªôt √≠t b√°o ch√≠) ƒë·ªÉ feed v√†o LLM (Llama-3.2-1B-Instruct).
        """
        seeds = self.search_seeds(question, top_k=seed_top_k)
        if not seeds:
            return {"seeds": [], "nodes": [], "context": "", "paths": []}

        subgraph_nodes = self.expand_subgraph(seeds, max_depth=depth, max_nodes=max_nodes)

        # S·∫Øp x·∫øp node trong subgraph theo importance: c·ªông ƒë·ªìng c√πng seed + PageRank
        def node_score(title: str) -> Tuple[int, float]:
            n = self.nodes[title]
            # ∆∞u ti√™n c√πng c·ªông ƒë·ªìng v·ªõi seed ƒë·∫ßu ti√™n
            same_comm = int(
                any(self.nodes[s].community_id == n.community_id for s in seeds if s in self.nodes)
            )
            return (same_comm, n.pagerank)

        selected_nodes = sorted(subgraph_nodes, key=node_score, reverse=True)

        # (tu·ª≥ ch·ªçn) tr√≠ch xu·∫•t c√°c ƒë∆∞·ªùng ƒëi multi-hop gi·ªØa c√°c seed
        paths: List[Dict] = []
        if include_paths:
            paths = self.extract_multi_hop_paths(seeds, max_depth=3)

        # Gh√©p ƒëo·∫°n context t·ª´ Wiki + m√¥ t·∫£ path
        parts: List[str] = []

        if paths:
            parts.append("[GRAPH PATHS]")
            for p in paths:
                node_chain = " -> ".join(p["path"])
                parts.append(f"- {p['source']} ~ {p['target']}: {node_chain}")
            parts.append("")  # d√≤ng tr·ªëng
        node_summaries: List[Dict] = []

        for title in selected_nodes:
            n = self.nodes[title]
            text = self.node_texts.get(title, {})
            intro = text.get("intro_text", "")
            if not intro:
                continue
            header = f"[NODE] {title} ({n.label})"
            meta = f"(PageRank={n.pagerank:.4f}, Community={n.community_id})"
            parts.append(f"{header} {meta}\n{intro}\n")
            node_summaries.append(
                {
                    "title": title,
                    "label": n.label,
                    "pagerank": n.pagerank,
                    "community_id": n.community_id,
                }
            )

        # Th√™m m·ªôt √≠t context t·ª´ b√°o ch√≠: ƒë∆°n gi·∫£n l√† nh·ªØng b√†i c√≥ t·ª´ kh√≥a trong ti√™u ƒë·ªÅ
        news_added = 0
        q_norm = normalize_for_match(question)
        for doc in self.news_docs:
            if news_added >= max_news:
                break
            title = doc.get("title", "")
            if not title:
                continue
            if any(tok in normalize_for_match(title) for tok in q_norm.split(" ") if tok):
                text = doc.get("text", "")
                if not text:
                    continue
                parts.append(f"[NEWS] {title}\n{text}\n")
                news_added += 1

        context = "\n".join(parts)
        return {
            "seeds": seeds,
            "nodes": node_summaries,
            "context": context,
            "paths": paths,
        }


if __name__ == "__main__":
    # Demo ƒë∆°n gi·∫£n: x√¢y index v√† build context cho 1 c√¢u h·ªèi
    print("--- üöÄ X√¢y d·ª±ng GraphRAGIndex v√† demo build context ---")
    index = GraphRAGIndex.from_files()
    question = "Quan h·ªá gi·ªØa vua Minh M·∫°ng v√† Gia Long l√† g√¨?"
    result = index.build_context_for_question(question)
    print("Seeds:", result["seeds"])
    print("S·ªë node trong context:", len(result["nodes"]))
    print("ƒê·ªô d√†i context (k√≠ t·ª±):", len(result["context"]))


