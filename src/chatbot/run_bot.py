import sys
import json
import torch
from neo4j import GraphDatabase
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- C·∫§U H√åNH ---
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASS = "12345678"
MODEL_ID = "Qwen/Qwen2.5-0.5B-Instruct"

# --- 1. KH·ªûI T·∫†O MODEL (CH·∫æ ƒê·ªò CPU SAFE MODE) ---
print(f"\n>>> ‚è≥ ƒêang kh·ªüi t·∫°o Model ({MODEL_ID})...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ‚ö†Ô∏è QUAN TR·ªåNG: √âp ch·∫°y CPU ƒë·ªÉ tr√°nh l·ªói "NDArray > 2**32" tr√™n Mac
    print("    - ƒêang c·∫•u h√¨nh ch·∫°y tr√™n CPU (Ch·∫ø ƒë·ªô ·ªïn ƒë·ªãnh cho Mac)...")
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        device_map="cpu",  # <--- KH√îNG D√ôNG "auto" hay "mps"
        torch_dtype=torch.float32, # <--- CPU ch·∫°y ·ªïn ƒë·ªãnh nh·∫•t v·ªõi float32
        low_cpu_mem_usage=True
    )
    print("    - Model ƒë√£ load th√†nh c√¥ng!")

except Exception as e:
    print(f"‚ùå L·ªói load model: {e}")
    sys.exit(1)

# --- 2. K·∫æT N·ªêI NEO4J ---
try:
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))
    driver.verify_connectivity()
    print("    - Neo4j ƒë√£ k·∫øt n·ªëi!")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi Neo4j: {e}")
    sys.exit(1)

# --- 3. C√ÅC H√ÄM TRUY V·∫§N ---

def query_summary(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    RETURN node.title as name, node.summary as summary
    LIMIT 1
    """
    with driver.session() as session:
        record = session.run(cypher, kw=keyword).single()
        if record and record["summary"]:
            return f"T√ìM T·∫ÆT V·ªÄ {record['name']}:\n{record['summary']}"
    return None

def query_relations(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    WITH node LIMIT 1
    MATCH (node)-[r]-(n1)
    WHERE NOT type(r) IN ['LI√äN_K·∫æT_T·ªöI']
    RETURN node.title AS center, type(r) AS rel_type, n1.title AS neighbor
    LIMIT 30
    """
    lines = []
    with driver.session() as session:
        for r in session.run(cypher, kw=keyword):
            r_type = r['rel_type'].replace('_', ' ').lower()
            lines.append(f"- {r['center']} l√† {r_type} c·ªßa {r['neighbor']}")
    return "\n".join(lines) if lines else None

# --- 4. ROUTER ---

def detect_intent_and_keyword(question):
    prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi sau v√† tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON duy nh·∫•t.
C√¢u h·ªèi: "{question}"

Y√™u c·∫ßu:
1. "intent": Ch·ªçn "SUMMARY" (ti·ªÉu s·ª≠, l√† ai) ho·∫∑c "RELATION" (quan h·ªá, cha con).
2. "keyword": T√™n nh√¢n v·∫≠t ch√≠nh.

V√≠ d·ª•: "Vua Minh M·∫°ng l√† ai?" -> {{"intent": "SUMMARY", "keyword": "Minh M·∫°ng"}}

Tr·∫£ v·ªÅ JSON:"""

    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt") # Kh√¥ng c·∫ßn .to(device) v√¨ ƒëang ·ªü CPU

    # Sinh JSON
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            max_new_tokens=64,
            do_sample=False, # Greedy decoding cho JSON
            pad_token_id=tokenizer.eos_token_id,
            attention_mask=inputs.attention_mask # S·ª≠a warning attention mask
        )
    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

    try:
        start = response.find("{")
        end = response.rfind("}") + 1
        if start != -1 and end != -1:
            json_str = response[start:end]
            return json.loads(json_str)
        else:
            return {"intent": "RELATION", "keyword": question}
    except:
        return {"intent": "RELATION", "keyword": question}

# --- 5. RAG GENERATOR ---

def generate_rag_response(question):
    # 1. Router
    analysis = detect_intent_and_keyword(question)
    intent = analysis.get("intent", "RELATION")
    keyword = analysis.get("keyword", question)

    print(f"\n[DEBUG] Intent: {intent} | Keyword: {keyword}")

    # 2. Retriever
    if intent == "SUMMARY":
        context = query_summary(keyword)
        if not context:
            context = query_relations(keyword)
            intent = "RELATION (Fallback)"
    else:
        context = query_relations(keyword)

    if not context:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu."

    # 3. Generator
    system_prompt = "B·∫°n l√† tr·ª£ l√Ω l·ªãch s·ª≠ Vi·ªát Nam. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. Tr·∫£ l·ªùi ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát."
    
    user_prompt = f"""TH√îNG TIN T·ª™ DATABASE ({intent}):
----------------
{context}
----------------

C√ÇU H·ªéI: {question}
TR·∫¢ L·ªúI:"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]
    
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask, # Fix warning
            max_new_tokens=300,
            temperature=0.3,
            do_sample=True, # Fix warning (temperature c·∫ßn do_sample=True)
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id
        )
    
    answer = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
    return answer.strip()

# --- 6. MAIN LOOP ---
def start_chat():
    print("\n" + "="*50)
    print("ü§ñ Chatbot S·ª¨ VI·ªÜT (Transformers CPU Mode)")
    print("‚ö†Ô∏è L∆∞u √Ω: Ch·∫°y tr√™n CPU s·∫Ω ch·∫≠m h∆°n GPU/MLX")
    print("="*50)
    
    while True:
        try:
            q = input("\nB·∫°n: ").strip()
            if q.lower() in ["exit", "quit", "tho√°t"]:
                print("Bot: T·∫°m bi·ªát!")
                break
            if not q: continue

            ans = generate_rag_response(q)
            print(f"Bot: {ans}")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ùå L·ªói Runtime: {e}")

if __name__ == "__main__":
    start_chat()