import sys
import json
from neo4j import GraphDatabase
from mlx_lm import load, generate

# --- CONFIG ---
NEO4J_URI = "bolt://localhost:7687"
NEO4J_USER = "neo4j"
NEO4J_PASS = "12345678"
# Model MLX Native (T·∫£i b·∫£n n√†y ƒë·ªÉ t·ªëi ∆∞u cho Mac)
MODEL_ID = "Qwen/Qwen3-0.6B-MLX-bf16" 

# --- 1. LOAD MLX MODEL ---
print("\n>>> ‚è≥ ƒêang kh·ªüi t·∫°o Model MLX (Si√™u t·ªëc cho Mac)...")
try:
    # Load model v√† tokenizer b·∫±ng th∆∞ vi·ªán mlx_lm
    model, tokenizer = load(MODEL_ID)
    print("    - Model ƒë√£ load th√†nh c√¥ng!")
except Exception as e:
    print(f"‚ùå L·ªói load MLX model: {e}")
    sys.exit(1)

# --- 2. CONNECT NEO4J ---
try:
    driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASS))
    driver.verify_connectivity()
    print("    - Neo4j ƒë√£ k·∫øt n·ªëi!")
except Exception as e:
    print(f"‚ùå L·ªói k·∫øt n·ªëi Neo4j: {e}")
    sys.exit(1)

# --- 3. QUERY FUNCTIONS ---

def query_summary(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    RETURN node.title as name, node.summary as summary
    LIMIT 1
    """
    with driver.session() as session:
        record = session.run(cypher, kw=keyword).single()
        if record and record["summary"]:
            return f"T√ìM T·∫ÆT V·ªÄ {record['name']}:\n{record['summary']}"
    return None

def query_relations(keyword):
    cypher = """
    CALL db.index.fulltext.queryNodes("title_index", $kw) YIELD node, score
    WHERE score > 0.6
    WITH node LIMIT 1
    MATCH (node)-[r]-(n1)
    WHERE NOT type(r) IN ['LI√äN_K·∫æT_T·ªöI']
    RETURN node.title AS center, type(r) AS rel_type, n1.title AS neighbor
    LIMIT 30
    """
    lines = []
    with driver.session() as session:
        for r in session.run(cypher, kw=keyword):
            lines.append(f"- {r['center']} --[{r['rel_type']}]--> {r['neighbor']}")
    return "\n".join(lines) if lines else None

# --- 4. INTENT DETECTOR (MLX) ---

def run_mlx(prompt: str, max_tokens=128):
    """Helper ƒë·ªÉ sinh text v·ªõi MLX, t·∫Øt verbose ƒë·ªÉ kh√¥ng in lung tung"""
    output = generate(
        model, 
        tokenizer, 
        prompt=prompt, 
        max_tokens=max_tokens, 
        verbose=True
    )
    return output

def detect_intent_and_keyword(question):
    prompt = f"""Ph√¢n t√≠ch c√¢u h·ªèi sau v√† tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON duy nh·∫•t.
C√¢u h·ªèi: "{question}"

Y√™u c·∫ßu:
1. "intent": Ch·ªçn "SUMMARY" (h·ªèi l√† ai, ti·ªÉu s·ª≠) ho·∫∑c "RELATION" (h·ªèi quan h·ªá, cha con).
2. "keyword": T√™n nh√¢n v·∫≠t ch√≠nh.

V√≠ d·ª•: "Vua Minh M·∫°ng l√† ai?" -> {{"intent": "SUMMARY", "keyword": "Minh M·∫°ng"}}

Tr·∫£ v·ªÅ JSON:"""

    # √Åp d·ª•ng chat template n·∫øu c√≥
    if hasattr(tokenizer, "apply_chat_template") and tokenizer.chat_template:
        messages = [{"role": "user", "content": prompt}]
        final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        final_prompt = prompt

    raw = run_mlx(final_prompt, max_tokens=100)

    # C·ªë g·∫Øng tr√≠ch xu·∫•t JSON t·ª´ ph·∫£n h·ªìi
    try:
        start = raw.find("{")
        end = raw.rfind("}") + 1
        if start != -1 and end != -1:
            json_str = raw[start:end]
            return json.loads(json_str)
        else:
            return {"intent": "RELATION", "keyword": question}
    except:
        return {"intent": "RELATION", "keyword": question}

# --- 5. RAG ANSWERING ---

def generate_rag_response(question):
    # 1. Router
    analysis = detect_intent_and_keyword(question)
    intent = analysis.get("intent", "RELATION")
    keyword = analysis.get("keyword", question)

    print(f"\n[DEBUG] Intent: {intent} | Keyword: {keyword}")

    # 2. Retriever
    if intent == "SUMMARY":
        context = query_summary(keyword)
        if not context:
            context = query_relations(keyword)
            intent = "RELATION (Fallback)"
    else:
        context = query_relations(keyword)

    if not context:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin trong c∆° s·ªü d·ªØ li·ªáu."

    # 3. Generator
    db_context = f"""TH√îNG TIN T·ª™ C∆† S·ªû D·ªÆ LI·ªÜU ({intent}):
---------------------
{context}
---------------------"""

    user_prompt = f"{db_context}\n\nD·ª±a v√†o th√¥ng tin tr√™n, h√£y tr·∫£ l·ªùi c√¢u h·ªèi: {question}\nTr·∫£ l·ªùi ng·∫Øn g·ªçn:"

    if hasattr(tokenizer, "apply_chat_template") and tokenizer.chat_template:
        messages = [
            {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω l·ªãch s·ª≠ Vi·ªát Nam trung th·ª±c. Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p."},
            {"role": "user", "content": user_prompt},
        ]
        final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    else:
        final_prompt = user_prompt

    # TƒÉng max_tokens cho c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
    answer = run_mlx(final_prompt, max_tokens=512)
    return answer.strip()

# --- 6. CHAT LOOP ---
def start_chat():
    print("\n" + "="*50)
    print("ü§ñ Chatbot L·ªãch s·ª≠ Vi·ªát Nam - MLX Optimized")
    print("="*50)

    while True:
        try:
            q = input("\nB·∫°n: ").strip()
            if q.lower() in ["exit", "quit", "tho√°t"]:
                print("Bot: T·∫°m bi·ªát!")
                break
            if not q: continue

            ans = generate_rag_response(q)
            print(f"Bot: {ans}")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"‚ùå L·ªói Runtime: {e}")

if __name__ == "__main__":
    start_chat()